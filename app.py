"""
AI-Generated Voice Detection API
FastAPI application for detecting AI-generated vs Human voices
Optimized for Render free tier (512MB RAM)
"""

import os
import base64
import io
import numpy as np
import librosa
from fastapi import FastAPI, HTTPException, Header, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import Optional

# ============== TensorFlow Optimizations ==============
# Set before importing TF
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF warnings
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN for consistent results

import tensorflow as tf

# Limit CPU threads for free tier
tf.config.threading.set_intra_op_parallelism_threads(2)
tf.config.threading.set_inter_op_parallelism_threads(2)

keras = tf.keras

# ============== Configuration ==============
API_KEY = os.environ.get("API_KEY", "sk_test_123456789")  # Set in Render environment

# Model backend selection: "keras", "tflite_int8"
MODEL_BACKEND = os.environ.get("MODEL_BACKEND", "tflite_int8").lower()

SAMPLE_RATE = 16000  # Must match training sample rate
N_LFCC = 40
N_FFT = 2048
HOP_LENGTH = 512
TARGET_TIME_STEPS = 312  # Fixed by model architecture
MAX_DURATION = 10.0  # seconds (flexible input, always output 312 steps)

SUPPORTED_LANGUAGES = ["Tamil", "English", "Hindi", "Malayalam", "Telugu"]

# Model paths
MODEL_PATHS = {
    "keras": "model/model.h5",
    "tflite_int8": "model/model_int8.tflite",
    "tflite_int8_hybrid": "model/model_int8_hybrid.tflite"
}

# ============== Model Architecture ==============
def build_cnn_model(input_shape=(N_LFCC, TARGET_TIME_STEPS, 1)):
    """
    CNN architecture for voice classification.
    Rebuilds the model to avoid Keras version compatibility issues.
    """
    from tensorflow.keras import layers, Model
    
    inputs = layers.Input(shape=input_shape, name='input')
    
    # Block 1
    x = layers.Conv2D(32, (3, 3), padding='same', name='conv1')(inputs)
    x = layers.BatchNormalization(name='bn1')(x)
    x = layers.Activation('relu', name='relu1')(x)
    x = layers.MaxPooling2D((2, 2), name='pool1')(x)
    x = layers.Dropout(0.25, name='dropout1')(x)
    
    # Block 2
    x = layers.Conv2D(64, (3, 3), padding='same', name='conv2')(x)
    x = layers.BatchNormalization(name='bn2')(x)
    x = layers.Activation('relu', name='relu2')(x)
    x = layers.MaxPooling2D((2, 2), name='pool2')(x)
    x = layers.Dropout(0.25, name='dropout2')(x)
    
    # Block 3
    x = layers.Conv2D(128, (3, 3), padding='same', name='conv3')(x)
    x = layers.BatchNormalization(name='bn3')(x)
    x = layers.Activation('relu', name='relu3')(x)
    x = layers.MaxPooling2D((2, 2), name='pool3')(x)
    x = layers.Dropout(0.25, name='dropout3')(x)
    
    # Block 4
    x = layers.Conv2D(256, (3, 3), padding='same', name='conv4')(x)
    x = layers.BatchNormalization(name='bn4')(x)
    x = layers.Activation('relu', name='relu4')(x)
    x = layers.GlobalAveragePooling2D(name='gap')(x)
    
    # Dense layers
    x = layers.Dense(128, activation='relu', name='dense1')(x)
    x = layers.Dropout(0.5, name='dropout4')(x)
    x = layers.Dense(64, activation='relu', name='dense2')(x)
    x = layers.Dropout(0.5, name='dropout5')(x)
    
    # Output
    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)
    
    model = Model(inputs=inputs, outputs=outputs, name='VoiceClassifierCNN')
    return model

# ============== Load Model ==============
print(f"Loading model with backend: {MODEL_BACKEND}")
model_path = MODEL_PATHS.get(MODEL_BACKEND, MODEL_PATHS["keras"])

if not os.path.exists(model_path):
    print(f"⚠ Model not found: {model_path}, falling back to Keras")
    MODEL_BACKEND = "keras"
    model_path = MODEL_PATHS["keras"]

model = None
interpreter = None
input_details = None
output_details = None

if MODEL_BACKEND.startswith("tflite"):
    # Load TFLite model
    print(f"Loading TFLite model: {model_path}")
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    print(f"✓ TFLite model loaded: {input_details[0]['shape']}")
else:
    # Load Keras model
    print(f"Loading Keras model: {model_path}")
    try:
        model = keras.models.load_model(model_path, compile=False)
        print("✓ Model loaded directly")
    except Exception as e:
        print(f"Direct loading failed: {e}")
        print("Rebuilding from architecture...")
        model = build_cnn_model()
        model.load_weights(model_path)
        print("✓ Model loaded via weights")
    
    model.trainable = False
    
    # Pre-warm model
    print("Warming up model...")
    _dummy_input = np.zeros((1, N_LFCC, TARGET_TIME_STEPS, 1), dtype=np.float32)
    _ = model(_dummy_input, training=False)
    print("✓ Model ready")

# Load normalization parameters
print("Loading normalization parameters...")
norm_params = np.load("model/normalization_params.npz")
MEAN = np.array(norm_params['mean'], dtype=np.float32)
STD = np.array(norm_params['std'], dtype=np.float32)
print(f"Normalization parameters loaded! Shape: MEAN={MEAN.shape}, STD={STD.shape}")

# ============== FastAPI App ==============
app = FastAPI(
    title="AI Voice Detection API",
    description="Detects whether a voice sample is AI-generated or Human",
    version="1.0.0"
)

# ============== Request/Response Models ==============
class VoiceDetectionRequest(BaseModel):
    language: str = Field(..., description="Language: Tamil/English/Hindi/Malayalam/Telugu")
    audioFormat: str = Field(..., description="Audio format (mp3)")
    audioBase64: str = Field(..., description="Base64-encoded MP3 audio")

class VoiceDetectionResponse(BaseModel):
    status: str
    language: str
    classification: str
    confidenceScore: float
    explanation: str

class ErrorResponse(BaseModel):
    status: str = "error"
    message: str

# ============== Feature Extraction ==============
def extract_lfcc_from_bytes(audio_bytes, sr=SAMPLE_RATE, n_lfcc=N_LFCC, 
                 n_fft=N_FFT, hop_length=HOP_LENGTH, max_duration=MAX_DURATION):
    """
    Extract LFCC-style cepstral features from audio bytes (in-memory).
    Uses MFCC computation for practical spoofing detection.
    Always returns exactly 312 time steps (model requirement).
    """
    try:
        # Load audio from bytes (no disk I/O)
        audio_buffer = io.BytesIO(audio_bytes)
        y, sr = librosa.load(audio_buffer, sr=sr, duration=max_duration)
        
        # Compute LFCC-style cepstral features (matching training exactly)
        lfcc = librosa.feature.mfcc(
            y=y, 
            sr=sr, 
            n_mfcc=n_lfcc,
            n_fft=n_fft,
            hop_length=hop_length
        )
        
        # Always pad or truncate to exactly TARGET_TIME_STEPS (312)
        if lfcc.shape[1] < TARGET_TIME_STEPS:
            # Pad with zeros
            pad_width = TARGET_TIME_STEPS - lfcc.shape[1]
            lfcc = np.pad(lfcc, ((0, 0), (0, pad_width)), mode='constant')
        else:
            # Truncate to exactly 312 steps
            lfcc = lfcc[:, :TARGET_TIME_STEPS]
        
        return lfcc
    except Exception as e:
        print(f"Error processing audio: {e}")
        return np.zeros((n_lfcc, TARGET_TIME_STEPS), dtype=np.float32)

def preprocess_audio_bytes(audio_bytes):
    """
    Extract LFCC-style cepstral features and prepare for model input.
    Matches exact preprocessing used during training.
    """
    # Extract LFCC-style features from bytes
    features = extract_lfcc_from_bytes(audio_bytes)
    
    # Ensure float32 dtype before normalization
    features = features.astype(np.float32)
    
    # Normalize using saved parameters (feature-wise normalization)
    # MEAN and STD are arrays of shape (40,), need to broadcast correctly
    mean = MEAN
    std = STD
    if mean.ndim == 1:
        mean = mean[:, np.newaxis]
    if std.ndim == 1:
        std = std[:, np.newaxis]
    
    features = (features - mean) / (std + 1e-8)
    
    # Reshape for CNN input: (batch, height, width, channels)
    features = features[np.newaxis, ..., np.newaxis]
    
    return features

def generate_explanation(confidence: float, is_ai: bool) -> str:
    """
    Generate a human-readable explanation for the classification.
    """
    if is_ai:
        if confidence > 0.9:
            return "Strong indicators of synthetic speech: unnatural pitch consistency, robotic intonation patterns, and artificial spectral characteristics detected"
        elif confidence > 0.75:
            return "Moderate AI speech indicators: irregular prosody patterns and synthetic voice artifacts detected"
        elif confidence > 0.6:
            return "Slight synthetic characteristics detected: minor artifacts in speech patterns suggest AI generation"
        else:
            return "Weak AI indicators detected: some subtle synthetic patterns present in the audio"
    else:
        if confidence > 0.9:
            return "Strong human voice characteristics: natural pitch variations, authentic breathing patterns, and organic speech dynamics detected"
        elif confidence > 0.75:
            return "Natural human speech patterns detected: consistent with organic voice production"
        elif confidence > 0.6:
            return "Predominantly human characteristics: natural prosody with minor inconsistencies"
        else:
            return "Voice appears human with some ambiguous characteristics"

# ============== API Endpoints ==============
@app.get("/")
async def root():
    return {"message": "AI Voice Detection API", "status": "running"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "model_type": "keras"
    }

@app.post("/api/voice-detection", response_model=VoiceDetectionResponse)
async def detect_voice(
    request: VoiceDetectionRequest,
    x_api_key: Optional[str] = Header(None, alias="x-api-key")
):
    """
    Detect whether a voice sample is AI-generated or Human.
    """
    # Validate API Key
    if not x_api_key or x_api_key != API_KEY:
        raise HTTPException(
            status_code=401,
            detail={"status": "error", "message": "Invalid API key or malformed request"}
        )
    
    # Validate language
    if request.language not in SUPPORTED_LANGUAGES:
        raise HTTPException(
            status_code=400,
            detail={"status": "error", "message": f"Unsupported language. Must be one of: {', '.join(SUPPORTED_LANGUAGES)}"}
        )
    
    # Validate audio format
    if request.audioFormat.lower() != "mp3":
        raise HTTPException(
            status_code=400,
            detail={"status": "error", "message": "Only MP3 audio format is supported"}
        )
    
    try:
        # Decode base64 audio
        audio_bytes = base64.b64decode(request.audioBase64)
        
        # Preprocess audio (in-memory, no disk I/O)
        features = preprocess_audio_bytes(audio_bytes)
        
        # Run inference based on model backend
        if MODEL_BACKEND.startswith("tflite"):
            # TFLite inference
            # Handle INT8 input quantization
            if input_details[0]['dtype'] == np.uint8:
                scale, zero_point = input_details[0]['quantization']
                # Quantize and clip to valid uint8 range [0, 255]
                # Without clipping, values outside the representable range
                # wrap via modular arithmetic, corrupting the input
                input_data = features / scale + zero_point
                input_data = np.nan_to_num(input_data, nan=0.0, posinf=255.0, neginf=0.0)
                input_data = np.clip(input_data, 0, 255).astype(np.uint8)
            else:
                input_data = features.astype(input_details[0]['dtype'])
            
            interpreter.set_tensor(input_details[0]['index'], input_data)
            interpreter.invoke()
            output_data = interpreter.get_tensor(output_details[0]['index'])
            
            # Handle INT8 output dequantization
            if output_details[0]['dtype'] == np.uint8:
                scale, zero_point = output_details[0]['quantization']
                output_data = (output_data.astype(np.float32) - zero_point) * scale
            
            prediction = float(output_data[0][0])
        else:
            # Keras inference
            prediction = model(features, training=False)[0][0].numpy()
        
        # Log prediction for debugging
        print(f"Raw prediction: {prediction:.4f}, Backend: {MODEL_BACKEND}, Language: {request.language}")
        
        # Determine classification
        # Model output: higher value = more likely AI_GENERATED
        # Note: Adjust threshold or flip logic if accuracy is consistently inverted
        is_ai = prediction >= 0.5
        classification = "AI_GENERATED" if is_ai else "HUMAN"
        
        # Confidence score (distance from decision boundary)
        confidence_score = float(prediction) if is_ai else float(1 - prediction)
        
        # Generate explanation
        explanation = generate_explanation(confidence_score, is_ai)
        
        return VoiceDetectionResponse(
            status="success",
            language=request.language,
            classification=classification,
            confidenceScore=round(confidence_score, 2),
            explanation=explanation
        )
                
    except base64.binascii.Error:
        raise HTTPException(
            status_code=400,
            detail={"status": "error", "message": "Invalid Base64 encoding"}
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail={"status": "error", "message": f"Error processing audio: {str(e)}"}
        )

# ============== Error Handlers ==============
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content=exc.detail if isinstance(exc.detail, dict) else {"status": "error", "message": str(exc.detail)}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=500,
        content={"status": "error", "message": "Internal server error"}
    )

# ============== Main ==============
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)

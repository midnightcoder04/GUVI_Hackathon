{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14811558,"sourceType":"datasetVersion","datasetId":9471445}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n============================================================================\nAI AUDIO DETECTION - TENSORFLOW IMPLEMENTATION\nLFCC + LCNN with State-of-the-Art Improvements\n============================================================================\n\nFeatures:\n1. RawBoost Data Augmentation (state-of-the-art for spoofing detection)\n2. True LCNN Architecture with Max-Feature-Map (MFM) activations\n3. Residual Connections for deeper networks\n4. Ensemble with CQT features for vocoder artifact detection\n5. Full TensorFlow 2.x implementation\n\nAuthor: AI Audio Detection System\nTarget: 95-100% accuracy on AI-generated audio detection\n============================================================================\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport soundfile as sf\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, callbacks\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, \n                            classification_report, roc_auc_score, roc_curve)\nfrom scipy import signal\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# GPU Configuration\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"✓ Using GPU: {gpus}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"✓ Using CPU\")\n\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nclass Config:\n    \"\"\"Configuration class for all hyperparameters.\"\"\"\n    \n    # Paths\n    DATA_PATH = '/kaggle/input/datasets/alphabeastaroq/ai-audio-detection-guvi/new_train'\n    \n    # Audio parameters\n    SAMPLE_RATE = 16000\n    DURATION = 5.0\n    MAX_LENGTH = int(SAMPLE_RATE * DURATION)\n    \n    # LFCC parameters\n    N_FILTER = 20\n    N_LFCC = 60  # 20 filters * 3 (with deltas)\n    N_FFT = 512\n    HOP_LENGTH = 160\n    WIN_LENGTH = 400\n    \n    # CQT parameters (for ensemble)\n    CQT_BINS = 84  # 7 octaves * 12 bins per octave\n    CQT_BINS_PER_OCTAVE = 12\n    CQT_FMIN = 32.7  # C1 note\n    \n    # Model parameters\n    BATCH_SIZE = 32\n    LEARNING_RATE = 0.0001\n    NUM_EPOCHS = 50\n    EARLY_STOP_PATIENCE = 10\n    \n    # Data split\n    TEST_SIZE = 0.15\n    VAL_SIZE = 0.15\n    \n    # Model architecture\n    NUM_CLASSES = 2\n    \n    # RawBoost parameters\n    RAWBOOST_ALGO = [3, 4, 5]  # Linear and nonlinear convolutive noise\n    RAWBOOST_PROB = 0.5  # Probability of applying augmentation\n\nconfig = Config()\n\n\n# ============================================================================\n# RAWBOOST DATA AUGMENTATION\n# ============================================================================\n\nclass RawBoost:\n    \"\"\"\n    RawBoost: State-of-the-art data augmentation for spoofing detection.\n    \n    Models linear and non-linear convolutive noise (microphone effects)\n    directly on the waveform. This is currently the best augmentation\n    technique for anti-spoofing systems.\n    \n    Reference: \"RawBoost: A Raw Data Boosting and Augmentation Method \n    Applied to Automatic Speaker Verification Anti-Spoofing\"\n    \"\"\"\n    \n    def __init__(self, sr=16000):\n        self.sr = sr\n    \n    def apply(self, audio, algo=3):\n        \"\"\"\n        Apply RawBoost augmentation.\n        \n        Args:\n            audio: Input waveform\n            algo: Algorithm selection\n                  3: Linear convolutive noise\n                  4: Nonlinear convolutive noise (sigmoid)\n                  5: Nonlinear convolutive noise (tanh)\n        \"\"\"\n        if algo == 3:\n            return self._linear_filter(audio)\n        elif algo == 4:\n            return self._nonlinear_filter_sigmoid(audio)\n        elif algo == 5:\n            return self._nonlinear_filter_tanh(audio)\n        else:\n            return audio\n    \n    def _linear_filter(self, audio):\n        \"\"\"Apply random linear filtering (simulates microphone response).\"\"\"\n        # Random IIR filter coefficients\n        b = np.random.randn(5)\n        a = np.random.randn(5)\n        a[0] = 1.0  # Ensure stability\n        \n        # Normalize to prevent overflow\n        b = b / np.sum(np.abs(b))\n        a = a / np.sum(np.abs(a))\n        \n        # Apply filter\n        try:\n            filtered = signal.lfilter(b, a, audio)\n            # Normalize\n            filtered = filtered / (np.max(np.abs(filtered)) + 1e-8)\n            return filtered.astype(np.float32)\n        except:\n            return audio\n    \n    def _nonlinear_filter_sigmoid(self, audio):\n        \"\"\"Apply nonlinear distortion using sigmoid.\"\"\"\n        # Random gain\n        gain = np.random.uniform(0.5, 2.0)\n        # Apply sigmoid nonlinearity\n        distorted = 2.0 / (1.0 + np.exp(-gain * audio)) - 1.0\n        return distorted.astype(np.float32)\n    \n    def _nonlinear_filter_tanh(self, audio):\n        \"\"\"Apply nonlinear distortion using tanh.\"\"\"\n        # Random gain\n        gain = np.random.uniform(0.5, 2.0)\n        # Apply tanh nonlinearity\n        distorted = np.tanh(gain * audio)\n        return distorted.astype(np.float32)\n    \n    def random_augment(self, audio, prob=0.5):\n        \"\"\"Randomly apply RawBoost augmentation.\"\"\"\n        if np.random.random() < prob:\n            algo = np.random.choice([3, 4, 5])\n            return self.apply(audio, algo)\n        return audio\n\n\n# ============================================================================\n# FEATURE EXTRACTION\n# ============================================================================\n\nclass FeatureExtractor:\n    \"\"\"Extract LFCC and CQT features from audio.\"\"\"\n    \n    def __init__(self, sr=16000, n_lfcc=60, n_fft=512, hop_length=160, \n                 win_length=400, n_filter=20):\n        self.sr = sr\n        self.n_lfcc = n_lfcc\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.n_filter = n_filter\n    \n    def extract_lfcc(self, audio):\n        \"\"\"\n        Extract Linear Frequency Cepstral Coefficients (LFCC).\n        \n        LFCC uses linear frequency scale (better for spoofing detection\n        than MFCC's mel scale which is optimized for ASR).\n        \"\"\"\n        try:\n            # Compute STFT\n            stft = librosa.stft(\n                audio, \n                n_fft=self.n_fft, \n                hop_length=self.hop_length,\n                win_length=self.win_length\n            )\n            magnitude = np.abs(stft)\n            \n            # Check for zeros\n            if np.max(magnitude) == 0:\n                # Return zero features if audio is silent\n                n_frames = int(len(audio) / self.hop_length)\n                return np.zeros((self.n_lfcc, n_frames))\n            \n            # Linear filterbank\n            linear_filters = librosa.filters.mel(\n                sr=self.sr,\n                n_fft=self.n_fft,\n                n_mels=self.n_filter,\n                fmin=0,\n                fmax=self.sr // 2,\n                htk=True,  # Use HTK formula for linear spacing\n                norm=None\n            )\n            \n            # Apply filterbank\n            filtered = np.dot(linear_filters, magnitude)\n            \n            # Log compression with safety\n            log_filtered = np.log(filtered + 1e-8)\n            \n            # Check for NaN/Inf\n            if not np.isfinite(log_filtered).all():\n                log_filtered = np.nan_to_num(log_filtered, nan=-8.0, posinf=0.0, neginf=-8.0)\n            \n            # DCT to get cepstral coefficients\n            lfcc = librosa.feature.mfcc(\n                S=log_filtered,\n                n_mfcc=self.n_filter,\n                dct_type=2,\n                norm='ortho'\n            )\n            \n            # Add delta and delta-delta features\n            lfcc_delta = librosa.feature.delta(lfcc)\n            lfcc_delta2 = librosa.feature.delta(lfcc, order=2)\n            \n            # Check deltas for NaN/Inf\n            if not np.isfinite(lfcc_delta).all():\n                lfcc_delta = np.nan_to_num(lfcc_delta, nan=0.0, posinf=0.0, neginf=0.0)\n            if not np.isfinite(lfcc_delta2).all():\n                lfcc_delta2 = np.nan_to_num(lfcc_delta2, nan=0.0, posinf=0.0, neginf=0.0)\n            \n            # Concatenate all features\n            lfcc_features = np.concatenate([lfcc, lfcc_delta, lfcc_delta2], axis=0)\n            \n            # Final safety check\n            if not np.isfinite(lfcc_features).all():\n                lfcc_features = np.nan_to_num(lfcc_features, nan=0.0, posinf=0.0, neginf=0.0)\n            \n            return lfcc_features\n            \n        except Exception as e:\n            # If LFCC fails, return zeros\n            print(f\"Warning: LFCC extraction failed, returning zeros: {e}\")\n            n_frames = int(len(audio) / self.hop_length)\n            return np.zeros((self.n_lfcc, n_frames))\n    \n    def extract_cqt(self, audio):\n        \"\"\"\n        Extract Constant-Q Transform (CQT) features.\n        \n        CQT is excellent for detecting pitch artifacts common in vocoders\n        (like ElevenLabs) because it has logarithmic frequency resolution,\n        matching musical pitch perception.\n        \"\"\"\n        try:\n            cqt = librosa.cqt(\n                audio,\n                sr=self.sr,\n                hop_length=self.hop_length,\n                n_bins=config.CQT_BINS,\n                bins_per_octave=config.CQT_BINS_PER_OCTAVE,\n                fmin=config.CQT_FMIN\n            )\n            \n            # Convert to log magnitude with safety check\n            cqt_mag = np.abs(cqt)\n            max_val = np.max(cqt_mag)\n            if max_val > 0:\n                cqt_db = librosa.amplitude_to_db(cqt_mag, ref=max_val)\n            else:\n                # If all zeros, create zero array\n                cqt_db = np.zeros_like(cqt_mag)\n            \n            # Check for NaN/Inf\n            if not np.isfinite(cqt_db).all():\n                cqt_db = np.nan_to_num(cqt_db, nan=0.0, posinf=0.0, neginf=-80.0)\n            \n            # Add deltas\n            cqt_delta = librosa.feature.delta(cqt_db)\n            cqt_delta2 = librosa.feature.delta(cqt_db, order=2)\n            \n            # Check deltas for NaN/Inf\n            if not np.isfinite(cqt_delta).all():\n                cqt_delta = np.nan_to_num(cqt_delta, nan=0.0, posinf=0.0, neginf=0.0)\n            if not np.isfinite(cqt_delta2).all():\n                cqt_delta2 = np.nan_to_num(cqt_delta2, nan=0.0, posinf=0.0, neginf=0.0)\n            \n            # Concatenate\n            cqt_features = np.concatenate([cqt_db, cqt_delta, cqt_delta2], axis=0)\n            \n            # Final safety check\n            if not np.isfinite(cqt_features).all():\n                cqt_features = np.nan_to_num(cqt_features, nan=0.0, posinf=0.0, neginf=-80.0)\n            \n            return cqt_features\n            \n        except Exception as e:\n            # If CQT fails, return zeros\n            print(f\"Warning: CQT extraction failed, returning zeros: {e}\")\n            return np.zeros((config.CQT_BINS * 3, int(len(audio) / self.hop_length)))\n\n\n# ============================================================================\n# MAX-FEATURE-MAP (MFM) ACTIVATION\n# ============================================================================\n\nclass MaxFeatureMap(layers.Layer):\n    \"\"\"\n    Max-Feature-Map (MFM) activation from the original LCNN paper.\n    \n    MFM acts as a feature selector, taking the max of two feature maps.\n    This is significantly better than ReLU for spoofing detection as it\n    performs implicit feature selection at each layer.\n    \n    Reference: \"Deep Speaker: an End-to-End Neural Speaker Embedding System\"\n    \"\"\"\n    \n    def __init__(self, **kwargs):\n        super(MaxFeatureMap, self).__init__(**kwargs)\n    \n    def call(self, inputs):\n        # Split input channels into two groups\n        split = tf.split(inputs, num_or_size_splits=2, axis=-1)\n        # Take max across the two groups\n        return tf.maximum(split[0], split[1])\n    \n    def compute_output_shape(self, input_shape):\n        # Output has half the channels\n        return input_shape[:-1] + (input_shape[-1] // 2,)\n\n\n# ============================================================================\n# RESIDUAL MFM BLOCK\n# ============================================================================\n\nclass ResidualMFMBlock(layers.Layer):\n    \"\"\"\n    Residual block with MFM activation.\n    \n    Combines the benefits of:\n    - Residual connections (gradient flow, deeper networks)\n    - MFM activation (feature selection)\n    \"\"\"\n    \n    def __init__(self, filters, kernel_size=3, **kwargs):\n        super(ResidualMFMBlock, self).__init__(**kwargs)\n        self.filters = filters\n        self.kernel_size = kernel_size\n        \n        # Double filters for MFM (will be halved after MFM)\n        self.conv1 = layers.Conv2D(\n            filters * 2, kernel_size, padding='same',\n            kernel_initializer='he_normal'\n        )\n        self.bn1 = layers.BatchNormalization()\n        self.mfm1 = MaxFeatureMap()\n        \n        self.conv2 = layers.Conv2D(\n            filters * 2, kernel_size, padding='same',\n            kernel_initializer='he_normal'\n        )\n        self.bn2 = layers.BatchNormalization()\n        self.mfm2 = MaxFeatureMap()\n        \n        # Projection shortcut if needed\n        self.projection = None\n    \n    def build(self, input_shape):\n        # Add projection if input channels != output channels\n        if input_shape[-1] != self.filters:\n            self.projection = layers.Conv2D(\n                self.filters, 1, padding='same',\n                kernel_initializer='he_normal'\n            )\n        super(ResidualMFMBlock, self).build(input_shape)\n    \n    def call(self, inputs, training=None):\n        # Main path\n        x = self.conv1(inputs)\n        x = self.bn1(x, training=training)\n        x = self.mfm1(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        x = self.mfm2(x)\n        \n        # Shortcut path\n        shortcut = inputs\n        if self.projection is not None:\n            shortcut = self.projection(inputs)\n        \n        # Residual connection\n        return x + shortcut\n\n\n# ============================================================================\n# TRUE LCNN WITH MFM AND RESIDUAL CONNECTIONS\n# ============================================================================\n\ndef build_lcnn_mfm_residual(input_shape, num_classes=2):\n    \"\"\"\n    Build True LCNN with MFM activations and residual connections.\n    \n    This is the authentic LCNN architecture from the original paper,\n    enhanced with residual connections for deeper training.\n    \n    Architecture:\n    - MFM activations instead of ReLU\n    - Residual connections for gradient flow\n    - Optimized for spoofing detection\n    \"\"\"\n    \n    inputs = layers.Input(shape=input_shape)\n    \n    # Reshape for 2D convolution\n    x = layers.Reshape((*input_shape, 1))(inputs)\n    \n    # Initial conv block with MFM\n    x = layers.Conv2D(64 * 2, 5, padding='same', \n                     kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = MaxFeatureMap()(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Residual MFM blocks\n    x = ResidualMFMBlock(64)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = ResidualMFMBlock(128)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = ResidualMFMBlock(256)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = ResidualMFMBlock(256)(x)\n    \n    # Global pooling\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    # Dense layers with MFM\n    x = layers.Dense(512 * 2, kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = MaxFeatureMap()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    x = layers.Dense(256 * 2, kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = MaxFeatureMap()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # Output layer\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name='LCNN_MFM_Residual')\n    \n    return model\n\n\n# ============================================================================\n# CQT-BASED MODEL FOR ENSEMBLE\n# ============================================================================\n\ndef build_cqt_model(input_shape, num_classes=2):\n    \"\"\"\n    Build CQT-based model for ensemble.\n    \n    CQT is excellent for detecting vocoder artifacts because:\n    - Logarithmic frequency resolution matches pitch perception\n    - Better at detecting harmonic structure artifacts\n    - Complements LFCC features\n    \"\"\"\n    \n    inputs = layers.Input(shape=input_shape)\n    \n    # Reshape for 2D convolution\n    x = layers.Reshape((*input_shape, 1))(inputs)\n    \n    # Initial conv block\n    x = layers.Conv2D(64 * 2, 5, padding='same',\n                     kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = MaxFeatureMap()(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    # Residual blocks\n    x = ResidualMFMBlock(96)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = ResidualMFMBlock(128)(x)\n    x = layers.MaxPooling2D(2)(x)\n    \n    x = ResidualMFMBlock(256)(x)\n    \n    # Global pooling\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    # Dense layers\n    x = layers.Dense(512 * 2, kernel_initializer='he_normal')(x)\n    x = layers.BatchNormalization()(x)\n    x = MaxFeatureMap()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    # Output\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    \n    model = models.Model(inputs=inputs, outputs=outputs, name='CQT_Model')\n    \n    return model\n\n\n# ============================================================================\n# ENSEMBLE MODEL\n# ============================================================================\n\nclass EnsembleModel:\n    \"\"\"\n    Ensemble combining LFCC and CQT models.\n    \n    Averages predictions from both models for final output.\n    This captures both spectral characteristics (LFCC) and\n    pitch/harmonic artifacts (CQT).\n    \"\"\"\n    \n    def __init__(self, lfcc_model, cqt_model):\n        self.lfcc_model = lfcc_model\n        self.cqt_model = cqt_model\n    \n    def predict(self, lfcc_features, cqt_features, weights=(0.6, 0.4)):\n        \"\"\"\n        Predict using ensemble.\n        \n        Args:\n            lfcc_features: LFCC input\n            cqt_features: CQT input\n            weights: Ensemble weights (lfcc_weight, cqt_weight)\n        \"\"\"\n        lfcc_pred = self.lfcc_model.predict(lfcc_features, verbose=0)\n        cqt_pred = self.cqt_model.predict(cqt_features, verbose=0)\n        \n        # Weighted average\n        ensemble_pred = weights[0] * lfcc_pred + weights[1] * cqt_pred\n        \n        return ensemble_pred\n    \n    def evaluate(self, lfcc_features, cqt_features, labels, weights=(0.6, 0.4)):\n        \"\"\"Evaluate ensemble performance.\"\"\"\n        predictions = self.predict(lfcc_features, cqt_features, weights)\n        pred_classes = np.argmax(predictions, axis=1)\n        true_classes = np.argmax(labels, axis=1)\n        \n        accuracy = accuracy_score(true_classes, pred_classes)\n        \n        return accuracy, predictions\n\n\n# ============================================================================\n# DATASET PREPARATION\n# ============================================================================\n\nclass AudioDataset:\n    \"\"\"Dataset class for loading and processing audio files.\"\"\"\n    \n    def __init__(self, file_paths, labels, feature_extractor, \n                 augmentor=None, max_length=80000):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.feature_extractor = feature_extractor\n        self.augmentor = augmentor\n        self.max_length = max_length\n    \n    def load_and_process(self, file_path, label, augment=False):\n        \"\"\"Load audio and extract features with robust error handling.\"\"\"\n        try:\n            # Load audio\n            audio, sr = librosa.load(file_path, sr=self.feature_extractor.sr)\n            \n            # Check for NaN or Inf values\n            if not np.isfinite(audio).all():\n                print(f\"Warning: Non-finite values in {file_path}, cleaning...\")\n                # Replace NaN with 0 and clip Inf values\n                audio = np.nan_to_num(audio, nan=0.0, posinf=1.0, neginf=-1.0)\n            \n            # Normalize audio to prevent overflow\n            max_val = np.abs(audio).max()\n            if max_val > 0:\n                audio = audio / max_val\n            \n            # Pad or truncate\n            if len(audio) > self.max_length:\n                audio = audio[:self.max_length]\n            else:\n                audio = np.pad(audio, (0, self.max_length - len(audio)))\n            \n            # Apply RawBoost augmentation if training\n            if augment and self.augmentor is not None:\n                audio = self.augmentor.random_augment(audio, prob=config.RAWBOOST_PROB)\n                # Check again after augmentation\n                if not np.isfinite(audio).all():\n                    audio = np.nan_to_num(audio, nan=0.0, posinf=1.0, neginf=-1.0)\n            \n            # Extract features\n            lfcc_features = self.feature_extractor.extract_lfcc(audio)\n            cqt_features = self.feature_extractor.extract_cqt(audio)\n            \n            # Verify features are finite\n            if not np.isfinite(lfcc_features).all():\n                lfcc_features = np.nan_to_num(lfcc_features, nan=0.0, posinf=1.0, neginf=-1.0)\n            if not np.isfinite(cqt_features).all():\n                cqt_features = np.nan_to_num(cqt_features, nan=0.0, posinf=1.0, neginf=-1.0)\n            \n            return lfcc_features.T, cqt_features.T, label\n            \n        except Exception as e:\n            # If any error occurs, raise it to be caught by create_tf_dataset\n            raise Exception(f\"Error processing {file_path}: {str(e)}\")\n    \n    def create_tf_dataset(self, batch_size=32, shuffle=True, augment=False, \n                          feature_type='lfcc'):\n        \"\"\"\n        Create TensorFlow dataset with MEMORY-EFFICIENT streaming.\n        \n        Args:\n            batch_size: Batch size\n            shuffle: Whether to shuffle\n            augment: Whether to apply augmentation\n            feature_type: 'lfcc' or 'cqt' - which features to use\n        \"\"\"\n        \n        # Don't load everything into RAM! Use generator instead\n        print(f\"Creating memory-efficient {feature_type.upper()} dataset for {len(self.file_paths)} files...\")\n        \n        def data_generator():\n            \"\"\"Generator that yields features on-the-fly.\"\"\"\n            for file_path, label in zip(self.file_paths, self.labels):\n                try:\n                    lfcc, cqt, lbl = self.load_and_process(file_path, label, augment)\n                    \n                    # Validate shapes\n                    if lfcc.shape[0] == 0 or cqt.shape[0] == 0:\n                        continue\n                    \n                    # Yield only the requested feature type\n                    if feature_type == 'lfcc':\n                        yield (lfcc.astype(np.float32), lbl)\n                    else:  # cqt\n                        yield (cqt.astype(np.float32), lbl)\n                except Exception as e:\n                    # Skip bad files silently during training\n                    continue\n        \n        # Get output shapes from first file\n        print(\"Determining feature dimensions from first file...\")\n        sample_lfcc, sample_cqt, sample_label = None, None, None\n        for fp, lbl in zip(self.file_paths, self.labels):\n            try:\n                sample_lfcc, sample_cqt, sample_label = self.load_and_process(fp, lbl, False)\n                if sample_lfcc.shape[0] > 0 and sample_cqt.shape[0] > 0:\n                    break\n            except:\n                continue\n        \n        if sample_lfcc is None:\n            raise ValueError(\"Could not load any valid files!\")\n        \n        lfcc_shape = sample_lfcc.shape\n        cqt_shape = sample_cqt.shape\n        \n        # Select feature shape based on type\n        if feature_type == 'lfcc':\n            feature_shape = lfcc_shape\n            n_features = lfcc_shape[1]  # 60 for LFCC\n        else:\n            feature_shape = cqt_shape\n            n_features = cqt_shape[1]  # 252 for CQT\n        \n        print(f\"✓ Feature shape determined: {feature_shape}\")\n        \n        # Create dataset from generator (MEMORY EFFICIENT!)\n        dataset = tf.data.Dataset.from_generator(\n            data_generator,\n            output_signature=(\n                tf.TensorSpec(shape=(None, n_features), dtype=tf.float32),\n                tf.TensorSpec(shape=(), dtype=tf.int32)\n            )\n        )\n        \n        # Pad sequences to same length within each batch\n        def pad_features(features, label):\n            \"\"\"Pad features to fixed length.\"\"\"\n            # Target length (adjust based on your audio duration)\n            target_len = 500  # ~5 seconds at 16kHz with hop_length=160\n            \n            # Pad or truncate features\n            feat_len = tf.shape(features)[0]\n            if feat_len > target_len:\n                features = features[:target_len, :]\n            else:\n                pad_len = target_len - feat_len\n                features = tf.pad(features, [[0, pad_len], [0, 0]])\n            \n            # One-hot encode label\n            label_onehot = tf.one_hot(label, depth=2)\n            \n            return features, label_onehot\n        \n        # Apply padding\n        dataset = dataset.map(pad_features, num_parallel_calls=tf.data.AUTOTUNE)\n        \n        # Cache first epoch (optional - comment out if still OOM)\n        # dataset = dataset.cache()\n        \n        if shuffle:\n            dataset = dataset.shuffle(buffer_size=min(1000, len(self.file_paths)))\n        \n        dataset = dataset.batch(batch_size, drop_remainder=False)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n        print(f\"✓ Memory-efficient {feature_type.upper()} dataset created\\n\")\n        \n        # For compatibility, also return None arrays (won't be loaded into RAM)\n        return dataset, None, None, None\n\n\n# ============================================================================\n# TRAINING UTILITIES\n# ============================================================================\n\ndef create_callbacks(model_name, patience=10):\n    \"\"\"Create training callbacks.\"\"\"\n    \n    # Early stopping\n    early_stop = callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=patience,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    # Model checkpoint\n    checkpoint = callbacks.ModelCheckpoint(\n        f'{model_name}_best.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n    \n    # Learning rate reduction\n    lr_scheduler = callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    )\n    \n    return [early_stop, checkpoint, lr_scheduler]\n\n\ndef plot_training_history(history, model_name):\n    \"\"\"Plot training history.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Accuracy\n    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n    ax1.set_title(f'{model_name} - Accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Loss\n    ax2.plot(history.history['loss'], label='Train Loss')\n    ax2.plot(history.history['val_loss'], label='Val Loss')\n    ax2.set_title(f'{model_name} - Loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend()\n    ax2.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef evaluate_model(model, X_test, y_test, model_name):\n    \"\"\"Comprehensive model evaluation.\"\"\"\n    \n    # Predictions\n    y_pred_proba = model.predict(X_test, verbose=0)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    \n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Sensitivity and Specificity\n    tn, fp, fn, tp = cm.ravel()\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    # Print results\n    print(f\"\\n{'='*60}\")\n    print(f\"{model_name} - Test Results\")\n    print(f\"{'='*60}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"AUC-ROC: {auc:.4f}\")\n    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n    print(f\"Specificity: {specificity:.4f}\")\n    print(f\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, \n                               target_names=['Fake', 'Real']))\n    \n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n               xticklabels=['Fake', 'Real'],\n               yticklabels=['Fake', 'Real'])\n    plt.title(f'{model_name} - Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # ROC curve\n    fpr, tpr, _ = roc_curve(y_test[:, 1], y_pred_proba[:, 1])\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.4f})')\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'{model_name} - ROC Curve')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return {\n        'accuracy': accuracy,\n        'auc': auc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'confusion_matrix': cm\n    }\n\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"AI AUDIO DETECTION - TENSORFLOW IMPLEMENTATION\")\n    print(\"LFCC-LCNN with State-of-the-Art Improvements\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Initialize components\n    print(\"Initializing components...\")\n    feature_extractor = FeatureExtractor(\n        sr=config.SAMPLE_RATE,\n        n_lfcc=config.N_LFCC,\n        n_fft=config.N_FFT,\n        hop_length=config.HOP_LENGTH,\n        win_length=config.WIN_LENGTH,\n        n_filter=config.N_FILTER\n    )\n    \n    rawboost = RawBoost(sr=config.SAMPLE_RATE)\n    \n    print(\"✓ Components initialized\\n\")\n    \n    # Load dataset\n    print(\"Loading dataset...\")\n    # This is a placeholder - adjust to your actual data loading\n    # You'll need to implement the data loading based on your directory structure\n    \n    \n    file_paths = []\n    labels = []\n    \n    for folder in ['DF', 'LA', 'PA', 'Eleven']:\n        # Real files\n        real_folder = os.path.join(config.DATA_PATH, folder, 'real')\n        if os.path.exists(real_folder):\n            real_files = [os.path.join(real_folder, f) for f in os.listdir(real_folder)\n                         if f.endswith(('.mp3', '.wav', '.flac'))]\n            file_paths.extend(real_files)\n            labels.extend([1] * len(real_files))\n        \n        # Fake files\n        fake_folder = os.path.join(config.DATA_PATH, folder, 'spoof')\n        if os.path.exists(fake_folder):\n            fake_files = [os.path.join(fake_folder, f) for f in os.listdir(fake_folder)\n                         if f.endswith(('.mp3', '.wav', '.flac'))]\n            file_paths.extend(fake_files)\n            labels.extend([0] * len(fake_files))\n    \n    # Split data\n    X_train, X_temp, y_train, y_temp = train_test_split(\n        file_paths, labels, test_size=(config.TEST_SIZE + config.VAL_SIZE),\n        random_state=SEED, stratify=labels\n    )\n    \n    X_val, X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp\n    )\n    \n    print(f\"✓ Dataset loaded:\")\n    print(f\"  Train: {len(X_train)} samples\")\n    print(f\"  Val: {len(X_val)} samples\")\n    print(f\"  Test: {len(X_test)} samples\\n\")\n    \n    # Create datasets with STREAMING (memory efficient!)\n    print(\"Creating TensorFlow datasets (streaming mode)...\")\n    \n    train_dataset_obj = AudioDataset(X_train, y_train, feature_extractor, \n                                     rawboost, config.MAX_LENGTH)\n    val_dataset_obj = AudioDataset(X_val, y_val, feature_extractor, \n                                   None, config.MAX_LENGTH)\n    test_dataset_obj = AudioDataset(X_test, y_test, feature_extractor,\n                                    None, config.MAX_LENGTH)\n    \n    # Create LFCC datasets\n    print(\"\\n\" + \"=\"*60)\n    print(\"CREATING LFCC DATASETS\")\n    print(\"=\"*60)\n    train_lfcc_dataset, _, _, _ = train_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, True, True, feature_type='lfcc')\n    val_lfcc_dataset, _, _, _ = val_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='lfcc')\n    test_lfcc_dataset, _, _, _ = test_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='lfcc')\n    \n    print(\"✓ LFCC datasets created (memory efficient)\\n\")\n    \n    # Calculate steps per epoch\n    train_steps = len(X_train) // config.BATCH_SIZE\n    val_steps = len(X_val) // config.BATCH_SIZE\n    test_steps = len(X_test) // config.BATCH_SIZE\n    \n    # Build LFCC model\n    print(\"=\"*60)\n    print(\"BUILDING LFCC MODEL\")\n    print(\"=\"*60)\n    print(\"Building LFCC-LCNN model with MFM and Residual connections...\")\n    # Fixed input shape for 5-second audio\n    lfcc_input_shape = (500, 60)  # 500 time steps, 60 LFCC coefficients\n    lfcc_model = build_lcnn_mfm_residual(lfcc_input_shape, config.NUM_CLASSES)\n    \n    lfcc_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(lfcc_model.summary())\n    print(\"✓ LFCC model built\\n\")\n    \n    # Train LFCC model with streaming data\n    print(\"=\"*60)\n    print(\"TRAINING LFCC MODEL\")\n    print(\"=\"*60)\n    lfcc_history = lfcc_model.fit(\n        train_lfcc_dataset,\n        validation_data=val_lfcc_dataset,\n        epochs=config.NUM_EPOCHS,\n        steps_per_epoch=train_steps,\n        validation_steps=val_steps,\n        callbacks=create_callbacks('lfcc_model', config.EARLY_STOP_PATIENCE),\n        verbose=1\n    )\n    \n    plot_training_history(lfcc_history, 'LFCC_Model')\n    print(\"✓ LFCC model trained\\n\")\n    \n    # Evaluate LFCC model (on streaming test set)\n    print(\"=\"*60)\n    print(\"EVALUATING LFCC MODEL\")\n    print(\"=\"*60)\n    lfcc_results = lfcc_model.evaluate(test_lfcc_dataset, steps=test_steps, verbose=1)\n    print(f\"LFCC Model - Test Loss: {lfcc_results[0]:.4f}\")\n    print(f\"LFCC Model - Test Accuracy: {lfcc_results[1]:.4f}\\n\")\n    \n    # Build CQT model\n    print(\"=\"*60)\n    print(\"BUILDING CQT MODEL\")\n    print(\"=\"*60)\n    cqt_input_shape = (500, 252)  # 500 time steps, 252 CQT coefficients\n    cqt_model = build_cqt_model(cqt_input_shape, config.NUM_CLASSES)\n    \n    cqt_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    print(\"✓ CQT model built\\n\")\n    \n    # Create CQT datasets\n    print(\"=\"*60)\n    print(\"CREATING CQT DATASETS\")\n    print(\"=\"*60)\n    train_cqt_dataset, _, _, _ = train_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, True, True, feature_type='cqt')\n    val_cqt_dataset, _, _, _ = val_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='cqt')\n    test_cqt_dataset, _, _, _ = test_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='cqt')\n    \n    print(\"✓ CQT datasets created\\n\")\n    \n    # Train CQT model\n    print(\"=\"*60)\n    print(\"TRAINING CQT MODEL\")\n    print(\"=\"*60)\n    cqt_history = cqt_model.fit(\n        train_cqt_dataset,\n        validation_data=val_cqt_dataset,\n        epochs=config.NUM_EPOCHS,\n        steps_per_epoch=train_steps,\n        validation_steps=val_steps,\n        callbacks=create_callbacks('cqt_model', config.EARLY_STOP_PATIENCE),\n        verbose=1\n    )\n    \n    plot_training_history(cqt_history, 'CQT_Model')\n    print(\"✓ CQT model trained\\n\")\n    \n    # Evaluate CQT model\n    print(\"=\"*60)\n    print(\"EVALUATING CQT MODEL\")\n    print(\"=\"*60)\n    cqt_results = cqt_model.evaluate(test_cqt_dataset, steps=test_steps, verbose=1)\n    print(f\"CQT Model - Test Loss: {cqt_results[0]:.4f}\")\n    print(f\"CQT Model - Test Accuracy: {cqt_results[1]:.4f}\\n\")\n    \n    # For ensemble evaluation, we need to collect predictions\n    print(\"=\"*60)\n    print(\"ENSEMBLE EVALUATION\")\n    print(\"=\"*60)\n    print(\"Collecting predictions for ensemble...\")\n    \n    # Recreate test datasets\n    test_lfcc_dataset, _, _, _ = test_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='lfcc')\n    test_cqt_dataset, _, _, _ = test_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='cqt')\n    \n    # Collect predictions in batches (memory efficient)\n    lfcc_preds = []\n    cqt_preds = []\n    true_labels = []\n    \n    # Get LFCC predictions\n    print(\"Getting LFCC predictions...\")\n    for features, labels in test_lfcc_dataset.take(test_steps):\n        pred = lfcc_model.predict(features, verbose=0)\n        lfcc_preds.append(pred)\n        true_labels.append(labels.numpy())\n    \n    # Get CQT predictions (need to recreate dataset)\n    print(\"Getting CQT predictions...\")\n    test_cqt_dataset, _, _, _ = test_dataset_obj.create_tf_dataset(\n        config.BATCH_SIZE, False, False, feature_type='cqt')\n    for features, labels in test_cqt_dataset.take(test_steps):\n        pred = cqt_model.predict(features, verbose=0)\n        cqt_preds.append(pred)\n    \n    lfcc_preds = np.vstack(lfcc_preds)\n    cqt_preds = np.vstack(cqt_preds)\n    true_labels = np.vstack(true_labels)\n    \n    print(f\"✓ Collected {len(lfcc_preds)} predictions\")\n    \n    # Find best ensemble weights\n    print(\"\\nOptimizing ensemble weights...\")\n    best_accuracy = 0\n    best_weights = (0.5, 0.5)\n    \n    for lfcc_weight in [0.3, 0.4, 0.5, 0.6, 0.7]:\n        cqt_weight = 1.0 - lfcc_weight\n        weights = (lfcc_weight, cqt_weight)\n        \n        # Weighted ensemble\n        ensemble_pred = weights[0] * lfcc_preds + weights[1] * cqt_preds\n        y_pred = np.argmax(ensemble_pred, axis=1)\n        y_true = np.argmax(true_labels, axis=1)\n        \n        accuracy = accuracy_score(y_true, y_pred)\n        print(f\"  Weights {weights}: Accuracy = {accuracy:.4f}\")\n        \n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_weights = weights\n    \n    print(f\"\\n✓ Best ensemble weights: {best_weights}\")\n    print(f\"✓ Best ensemble accuracy: {best_accuracy:.4f}\")\n    \n    # Final ensemble predictions\n    ensemble_predictions = best_weights[0] * lfcc_preds + best_weights[1] * cqt_preds\n    y_pred = np.argmax(ensemble_predictions, axis=1)\n    y_true = np.argmax(true_labels, axis=1)\n    \n    ensemble_auc = roc_auc_score(true_labels, ensemble_predictions)\n    cm = confusion_matrix(y_true, y_pred)\n    \n    print(f\"\\nFinal Ensemble Results:\")\n    print(f\"  Accuracy: {best_accuracy:.4f}\")\n    print(f\"  AUC-ROC: {ensemble_auc:.4f}\")\n    print(f\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    # Plot ensemble confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n               xticklabels=['Fake', 'Real'],\n               yticklabels=['Fake', 'Real'])\n    plt.title('Ensemble Model - Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig('ensemble_confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # Save models\n    print(\"\\nSaving models...\")\n    lfcc_model.save('lfcc_model_final.h5')\n    cqt_model.save('cqt_model_final.h5')\n    \n    # Save ensemble weights\n    import json\n    with open('ensemble_weights.json', 'w') as f:\n        json.dump({\n            'lfcc_weight': best_weights[0],\n            'cqt_weight': best_weights[1],\n            'accuracy': float(best_accuracy),\n            'auc': float(ensemble_auc)\n        }, f, indent=4)\n    \n    print(\"✓ Models saved\\n\")\n    \n    print(\"=\"*80)\n    print(\"TRAINING COMPLETE!\")\n    print(\"=\"*80)\n    print(f\"\\nResults Summary:\")\n    print(f\"  LFCC Model: {lfcc_results['accuracy']:.4f} accuracy\")\n    print(f\"  CQT Model: {cqt_results['accuracy']:.4f} accuracy\")\n    print(f\"  Ensemble: {best_accuracy:.4f} accuracy\")\n    print(f\"\\nFiles created:\")\n    print(f\"  - lfcc_model_final.h5\")\n    print(f\"  - cqt_model_final.h5\")\n    print(f\"  - ensemble_weights.json\")\n    print(f\"  - Various plots and metrics\")\n    \n    print(\"\\n✓ Setup complete! Add your data loading code above to start training.\\n\")\n\n\nif __name__ == \"__main__\":\n    # Note: Uncomment the main() call after adding your data loading code\n    main()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"TENSORFLOW IMPLEMENTATION READY\")\n    print(\"=\"*80)\n    print(\"\\nKey Features Implemented:\")\n    print(\"  ✓ RawBoost Data Augmentation\")\n    print(\"  ✓ True LCNN with Max-Feature-Map (MFM) activations\")\n    print(\"  ✓ Residual Connections for deeper networks\")\n    print(\"  ✓ Ensemble with CQT features\")\n    print(\"  ✓ Complete training and evaluation pipeline\")\n    print(\"\\nNext Steps:\")\n    print(\"  1. Add your data loading code in the main() function\")\n    print(\"  2. Uncomment the main() call at the bottom\")\n    print(\"  3. Run the script to train your models\")\n    print(\"=\"*80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T02:37:07.079182Z","iopub.execute_input":"2026-02-13T02:37:07.079931Z","execution_failed":"2026-02-13T02:38:44.531Z"}},"outputs":[{"name":"stdout","text":"✓ Using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n\n================================================================================\nAI AUDIO DETECTION - TENSORFLOW IMPLEMENTATION\nLFCC-LCNN with State-of-the-Art Improvements\n================================================================================\n\nInitializing components...\n✓ Components initialized\n\nLoading dataset...\n✓ Dataset loaded:\n  Train: 8400 samples\n  Val: 1800 samples\n  Test: 1800 samples\n\nCreating TensorFlow datasets (streaming mode)...\n\n============================================================\nCREATING LFCC DATASETS\n============================================================\nCreating memory-efficient LFCC dataset for 8400 files...\nDetermining feature dimensions from first file...\n✓ Feature shape determined: (501, 60)\n✓ Memory-efficient LFCC dataset created\n\nCreating memory-efficient LFCC dataset for 1800 files...\nDetermining feature dimensions from first file...\n✓ Feature shape determined: (501, 60)\n✓ Memory-efficient LFCC dataset created\n\nCreating memory-efficient LFCC dataset for 1800 files...\nDetermining feature dimensions from first file...\n✓ Feature shape determined: (501, 60)\n✓ Memory-efficient LFCC dataset created\n\n✓ LFCC datasets created (memory efficient)\n\n============================================================\nBUILDING LFCC MODEL\n============================================================\nBuilding LFCC-LCNN model with MFM and Residual connections...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"LCNN_MFM_Residual\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LCNN_MFM_Residual\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_5 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_55 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │         \u001b[38;5;34m3,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_55          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │           \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_55              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mMaxFeatureMap\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_20 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_20           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │       \u001b[38;5;34m148,736\u001b[0m │\n│ (\u001b[38;5;33mResidualMFMBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_21 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_21           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m453,248\u001b[0m │\n│ (\u001b[38;5;33mResidualMFMBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_22 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_22           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │     \u001b[38;5;34m1,807,616\u001b[0m │\n│ (\u001b[38;5;33mResidualMFMBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_23 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_23           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │     \u001b[38;5;34m2,364,416\u001b[0m │\n│ (\u001b[38;5;33mResidualMFMBlock\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_5      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m263,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_64          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_64              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mMaxFeatureMap\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_65          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_65              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mMaxFeatureMap\u001b[0m)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_55          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_55              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxFeatureMap</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_20           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">148,736</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualMFMBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_21           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">453,248</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualMFMBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_22           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,807,616</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualMFMBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ residual_mfm_block_23           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,364,416</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualMFMBlock</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_5      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_64          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_64              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxFeatureMap</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_65          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_feature_map_65              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxFeatureMap</span>)                 │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,310,338\u001b[0m (20.26 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,310,338</span> (20.26 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,301,378\u001b[0m (20.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,301,378</span> (20.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,960\u001b[0m (35.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,960</span> (35.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n✓ LFCC model built\n\n============================================================\nTRAINING LFCC MODEL\n============================================================\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1770950312.816349     110 service.cc:152] XLA service 0x7f95c4003a70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1770950312.816392     110 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1770950312.816397     110 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1770950315.387404     110 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14789581,"sourceType":"datasetVersion","datasetId":9415542}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCOMPLETE XGBOOST PIPELINE - MINIMAL VERSION\nRun this as a single Kaggle notebook cell by cell\n\nDataset: ASINS Voice Database\nGoal: Train ultra-fast XGBoost model for AI vs Human voice detection\nExpected: 96-98% accuracy, 5-10ms inference\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:12:51.785218Z","iopub.execute_input":"2026-02-10T18:12:51.785457Z","iopub.status.idle":"2026-02-10T18:12:51.792833Z","shell.execute_reply.started":"2026-02-10T18:12:51.785438Z","shell.execute_reply":"2026-02-10T18:12:51.792228Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\nCOMPLETE XGBOOST PIPELINE - MINIMAL VERSION\\nRun this as a single Kaggle notebook cell by cell\\n\\nDataset: ASINS Voice Database\\nGoal: Train ultra-fast XGBoost model for AI vs Human voice detection\\nExpected: 96-98% accuracy, 5-10ms inference\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Install Dependencies\n# ============================================================================\n!pip install xgboost lightgbm -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:12:51.794053Z","iopub.execute_input":"2026-02-10T18:12:51.794250Z","iopub.status.idle":"2026-02-10T18:12:55.714858Z","shell.execute_reply.started":"2026-02-10T18:12:51.794232Z","shell.execute_reply":"2026-02-10T18:12:55.714065Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: Imports\n# ============================================================================\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom tqdm import tqdm\nimport gc\nimport pickle\nimport time\n\nprint(\"âœ“ All imports successful\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:12:55.715910Z","iopub.execute_input":"2026-02-10T18:12:55.716176Z","iopub.status.idle":"2026-02-10T18:12:58.305485Z","shell.execute_reply.started":"2026-02-10T18:12:55.716146Z","shell.execute_reply":"2026-02-10T18:12:58.304543Z"}},"outputs":[{"name":"stdout","text":"âœ“ All imports successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: Configuration\n# ============================================================================\n# Dataset paths (adjust if needed)\nBASE_PATH = '/kaggle/input/asins-voice/build'\nREAL_CLIPS_PATH = f'{BASE_PATH}/clips'\nAI_CLIPS_PATH = f'{BASE_PATH}/clips_AI'\n\n# Feature extraction parameters\nSAMPLE_RATE = 16000\nN_LFCC = 40\nN_FFT = 2048\nHOP_LENGTH = 512\nMAX_DURATION = 10\n\n# Dataset size\nDATASET_SIZE = 136000  # 5K real + 5K AI\nTEST_SIZE = 0.2\nRANDOM_STATE = 42\n\nprint(\"âœ“ Configuration set\")\nprint(f\"Dataset size: {DATASET_SIZE} samples\")\nprint(f\"Feature extraction: {N_LFCC} LFCC, {SAMPLE_RATE}Hz, {MAX_DURATION}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:22.747144Z","iopub.execute_input":"2026-02-10T18:24:22.747462Z","iopub.status.idle":"2026-02-10T18:24:22.752920Z","shell.execute_reply.started":"2026-02-10T18:24:22.747438Z","shell.execute_reply":"2026-02-10T18:24:22.752013Z"}},"outputs":[{"name":"stdout","text":"âœ“ Configuration set\nDataset size: 136000 samples\nFeature extraction: 40 LFCC, 16000Hz, 10s\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: Dataset Preparation\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 1: DATASET PREPARATION\")\nprint(\"=\"*80)\n\n# Collect audio files\nprint(\"\\n[1/5] Collecting audio files...\")\nreal_files = [f for f in os.listdir(REAL_CLIPS_PATH) if f.endswith(('.mp3', '.wav', '.ogg', '.flac'))]\nai_files = [f for f in os.listdir(AI_CLIPS_PATH) if f.endswith(('.mp3', '.wav', '.ogg', '.flac'))]\n\nprint(f\"Found {len(real_files)} real files\")\nprint(f\"Found {len(ai_files)} AI files\")\n\n# Find matched pairs\nprint(\"\\n[2/5] Finding matched pairs...\")\nreal_set = set(real_files)\nai_set = set(ai_files)\nmatched_files = list(real_set.intersection(ai_set))\n\nprint(f\"Found {len(matched_files)} matched pairs\")\n\n# Sample balanced subset\nprint(\"\\n[3/5] Sampling balanced subset...\")\nnp.random.seed(RANDOM_STATE)\nsamples_per_class = DATASET_SIZE // 2\n\nif len(matched_files) < samples_per_class:\n    print(f\"âš  Warning: Only {len(matched_files)} pairs available, using all\")\n    samples_per_class = len(matched_files)\n\nselected_files = np.random.choice(matched_files, size=samples_per_class, replace=False)\n\n# Create dataframe\nprint(\"\\n[4/5] Creating dataset...\")\ndata = []\nfor filename in selected_files:\n    data.append({\n        'filename': filename,\n        'filepath': os.path.join(REAL_CLIPS_PATH, filename),\n        'label': 0,\n        'label_text': 'REAL'\n    })\n    data.append({\n        'filename': filename,\n        'filepath': os.path.join(AI_CLIPS_PATH, filename),\n        'label': 1,\n        'label_text': 'AI_GENERATED'\n    })\n\ndf = pd.DataFrame(data)\ndf = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\nprint(f\"Total samples: {len(df)}\")\nprint(f\"\\nClass distribution:\")\nprint(df['label_text'].value_counts())\n\n# Train-test split\nprint(\"\\n[5/5] Creating train-test split...\")\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=TEST_SIZE, \n    random_state=RANDOM_STATE,\n    stratify=df['label']\n)\n\nprint(f\"Train: {len(train_df)} samples\")\nprint(f\"Test: {len(test_df)} samples\")\nprint(\"âœ“ Dataset preparation complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:27.087490Z","iopub.execute_input":"2026-02-10T18:24:27.087756Z","iopub.status.idle":"2026-02-10T18:24:27.517773Z","shell.execute_reply.started":"2026-02-10T18:24:27.087735Z","shell.execute_reply":"2026-02-10T18:24:27.517031Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTEP 1: DATASET PREPARATION\n================================================================================\n\n[1/5] Collecting audio files...\nFound 68030 real files\nFound 68030 AI files\n\n[2/5] Finding matched pairs...\nFound 68030 matched pairs\n\n[3/5] Sampling balanced subset...\n\n[4/5] Creating dataset...\nTotal samples: 136000\n\nClass distribution:\nlabel_text\nREAL            68000\nAI_GENERATED    68000\nName: count, dtype: int64\n\n[5/5] Creating train-test split...\nTrain: 108800 samples\nTest: 27200 samples\nâœ“ Dataset preparation complete!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Feature Extraction Functions\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 2: FEATURE EXTRACTION\")\nprint(\"=\"*80)\n\ndef extract_lfcc(audio_path):\n    \"\"\"Extract LFCC features from audio file.\"\"\"\n    try:\n        y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=MAX_DURATION)\n        \n        lfcc = librosa.feature.mfcc(\n            y=y, \n            sr=sr, \n            n_mfcc=N_LFCC,\n            n_fft=N_FFT,\n            hop_length=HOP_LENGTH\n        )\n        \n        target_length = int(MAX_DURATION * sr / HOP_LENGTH)\n        \n        if lfcc.shape[1] < target_length:\n            pad_width = target_length - lfcc.shape[1]\n            lfcc = np.pad(lfcc, ((0, 0), (0, pad_width)), mode='constant')\n        else:\n            lfcc = lfcc[:, :target_length]\n        \n        return lfcc\n    \n    except Exception as e:\n        print(f\"Error: {audio_path}: {e}\")\n        target_length = int(MAX_DURATION * SAMPLE_RATE / HOP_LENGTH)\n        return np.zeros((N_LFCC, target_length))\n\ndef extract_statistical_features(lfcc_matrix):\n    \"\"\"\n    Extract statistical features from LFCC.\n    This is MUCH faster than using raw LFCC for XGBoost.\n    \n    Input: (n_lfcc, time_steps) e.g., (40, 313)\n    Output: (n_features,) e.g., (365,)\n    \"\"\"\n    feat = []\n    \n    # Per-coefficient statistics (40 coefficients Ã— 9 stats = 360 features)\n    feat.extend(np.mean(lfcc_matrix, axis=1))       # 40\n    feat.extend(np.std(lfcc_matrix, axis=1))        # 40\n    feat.extend(np.min(lfcc_matrix, axis=1))        # 40\n    feat.extend(np.max(lfcc_matrix, axis=1))        # 40\n    feat.extend(np.percentile(lfcc_matrix, 25, axis=1))  # 40\n    feat.extend(np.percentile(lfcc_matrix, 50, axis=1))  # 40\n    feat.extend(np.percentile(lfcc_matrix, 75, axis=1))  # 40\n    feat.extend(np.max(lfcc_matrix, axis=1) - np.min(lfcc_matrix, axis=1))  # 40 (range)\n    feat.extend(np.percentile(lfcc_matrix, 75, axis=1) - np.percentile(lfcc_matrix, 25, axis=1))  # 40 (IQR)\n    \n    # Overall statistics (5 features)\n    feat.extend([\n        np.mean(lfcc_matrix),\n        np.std(lfcc_matrix),\n        np.min(lfcc_matrix),\n        np.max(lfcc_matrix),\n        np.median(lfcc_matrix)\n    ])\n    \n    return np.array(feat)\n\nprint(\"âœ“ Feature extraction functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:36.501456Z","iopub.execute_input":"2026-02-10T18:24:36.502533Z","iopub.status.idle":"2026-02-10T18:24:36.516540Z","shell.execute_reply.started":"2026-02-10T18:24:36.502483Z","shell.execute_reply":"2026-02-10T18:24:36.515736Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTEP 2: FEATURE EXTRACTION\n================================================================================\nâœ“ Feature extraction functions defined\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: Extract Features for Training Set\n# ============================================================================\nprint(\"\\n[1/2] Extracting TRAINING features...\")\nprint(\"This will take 10-20 minutes...\")\n\nX_train = []\ny_train = []\n\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Train\"):\n    # Extract LFCC\n    lfcc = extract_lfcc(row['filepath'])\n    \n    # Extract statistical features\n    features = extract_statistical_features(lfcc)\n    \n    X_train.append(features)\n    y_train.append(row['label'])\n    \n    # Clear memory periodically\n    if (idx + 1) % 1000 == 0:\n        gc.collect()\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\nprint(f\"âœ“ Training features extracted: {X_train.shape}\")\nprint(f\"  Samples: {X_train.shape[0]}\")\nprint(f\"  Features per sample: {X_train.shape[1]}\")\n\n# Save checkpoint\nnp.savez_compressed('features_train_xgb.npz', X=X_train, y=y_train)\nprint(\"âœ“ Saved checkpoint: features_train_xgb.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T18:24:40.141973Z","iopub.execute_input":"2026-02-10T18:24:40.142227Z"}},"outputs":[{"name":"stdout","text":"\n[1/2] Extracting TRAINING features...\nThis will take 10-20 minutes...\n","output_type":"stream"},{"name":"stderr","text":"Train:  24%|â–ˆâ–ˆâ–       | 26253/108800 [11:56<29:52, 46.04it/s]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: Extract Features for Test Set\n# ============================================================================\nprint(\"\\n[2/2] Extracting TEST features...\")\n\nX_test = []\ny_test = []\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Test\"):\n    lfcc = extract_lfcc(row['filepath'])\n    features = extract_statistical_features(lfcc)\n    \n    X_test.append(features)\n    y_test.append(row['label'])\n    \n    if (idx + 1) % 1000 == 0:\n        gc.collect()\n\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\nprint(f\"âœ“ Test features extracted: {X_test.shape}\")\n\n# Save checkpoint\nnp.savez_compressed('features_test_xgb.npz', X=X_test, y=y_test)\nprint(\"âœ“ Saved checkpoint: features_test_xgb.npz\")\n\nprint(\"\\nâœ“ Feature extraction complete!\")\nprint(f\"  Train: {X_train.shape}\")\nprint(f\"  Test: {X_test.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Train XGBoost Model\n# ============================================================================\nimport lightgbm as lgb\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 3: TRAIN XGBOOST MODEL\")\nprint(\"=\"*80)\n\n# XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 1,\n    'gamma': 0,\n    'reg_alpha': 0.1,\n    'reg_lambda': 1,\n    'tree_method': 'hist',\n    'random_state': 42,\n    'n_jobs': -1,\n    'eval_metric': 'logloss'\n}\n\nprint(\"\\nTraining XGBoost with parameters:\")\nfor key, value in params.items():\n    print(f\"  {key}: {value}\")\n\n# Train model\nprint(\"\\nTraining...\")\nstart_time = time.time()\n\nmodel = xgb.XGBClassifier(**params)\nmodel.fit(\n    X_train, \n    y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=True\n)\n\ntraining_time = time.time() - start_time\n\nprint(f\"\\nâœ“ Training completed in {training_time:.2f} seconds ({training_time/60:.1f} minutes)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: Evaluate Model\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 4: EVALUATION\")\nprint(\"=\"*80)\n\n# Predictions\nprint(\"\\nMaking predictions on test set...\")\ny_pred_proba = model.predict_proba(X_test)[:, 1]\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nðŸŽ¯ Accuracy: {accuracy*100:.2f}%\")\n\n# Classification report\nprint(\"\\nðŸ“‹ Classification Report:\")\nprint(classification_report(y_test, y_pred, \n                          target_names=['REAL', 'AI_GENERATED'],\n                          digits=4))\n\n# Confusion matrix\nprint(\"\\nðŸ“Š Confusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(f\"\\nTrue Negatives (REAL â†’ REAL): {cm[0][0]}\")\nprint(f\"False Positives (REAL â†’ AI): {cm[0][1]}\")\nprint(f\"False Negatives (AI â†’ REAL): {cm[1][0]}\")\nprint(f\"True Positives (AI â†’ AI): {cm[1][1]}\")\n\n# Speed benchmark\nprint(\"\\nâš¡ Speed Benchmark:\")\nn_samples = 100\ntest_samples = X_test[:n_samples]\n\nstart_time = time.time()\nfor i in range(n_samples):\n    _ = model.predict_proba(test_samples[i:i+1])\navg_inference_time = (time.time() - start_time) / n_samples * 1000\n\nprint(f\"  Average inference time: {avg_inference_time:.2f} ms/sample\")\nprint(f\"  Throughput: {1000/avg_inference_time:.0f} samples/second\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Save Model\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STEP 5: SAVE MODEL\")\nprint(\"=\"*80)\n\n# Save XGBoost model\nmodel.save_model('model_xgboost_full.json')\nmodel.save_model('model_xgboost_full.ubj')  # Fastest format\n\n# Save as pickle for compatibility\nwith open('model_xgboost_full.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Calculate sizes\nimport os\njson_size = os.path.getsize('model_xgboost.json') / (1024 * 1024)\npkl_size = os.path.getsize('model_xgboost.pkl') / (1024 * 1024)\n\nprint(f\"âœ“ Models saved:\")\nprint(f\"  model_xgboost.json ({json_size:.2f} MB)\")\nprint(f\"  model_xgboost.ubj (fastest loading)\")\nprint(f\"  model_xgboost.pkl ({pkl_size:.2f} MB)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Final Summary\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPLETE! âœ…\")\nprint(\"=\"*80)\n\nprint(f\"\\nðŸ“Š FINAL RESULTS:\")\nprint(f\"  Accuracy:         {accuracy*100:.2f}%\")\nprint(f\"  Model size:       {json_size:.2f} MB\")\nprint(f\"  Inference speed:  {avg_inference_time:.2f} ms\")\nprint(f\"  Training time:    {training_time/60:.1f} minutes\")\n\nprint(f\"\\nðŸ’¾ SAVED FILES:\")\nprint(f\"  â€¢ model_xgboost.json (deploy this)\")\nprint(f\"  â€¢ model_xgboost.ubj (fastest)\")\nprint(f\"  â€¢ model_xgboost.pkl (Python)\")\nprint(f\"  â€¢ features_train_xgb.npz (checkpoint)\")\nprint(f\"  â€¢ features_test_xgb.npz (checkpoint)\")\n\nprint(f\"\\nðŸš€ USAGE IN PRODUCTION:\")\nprint(\"\"\"\nimport xgboost as xgb\nimport numpy as np\nimport librosa\n\n# Load model\nmodel = xgb.Booster()\nmodel.load_model('model_xgboost.json')\n\n# Extract features from new audio\ny, sr = librosa.load('audio.mp3', sr=16000, duration=10)\nlfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\nfeatures = extract_statistical_features(lfcc)\n\n# Predict\ndmatrix = xgb.DMatrix(features.reshape(1, -1))\nprediction = model.predict(dmatrix)[0]\nclassification = \"AI_GENERATED\" if prediction > 0.5 else \"REAL\"\nconfidence = prediction if prediction > 0.5 else 1 - prediction\n\nprint(f\"Classification: {classification}\")\nprint(f\"Confidence: {confidence:.2f}\")\n\"\"\")\n\nprint(\"\\nâœ… XGBoost model ready for deployment!\")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 12: Quick Test on Random Sample\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"BONUS: QUICK TEST\")\nprint(\"=\"*80)\n\n# Test on a random sample\nrandom_idx = np.random.randint(0, len(test_df))\ntest_row = test_df.iloc[random_idx]\n\nprint(f\"\\nTesting on: {test_row['filename']}\")\nprint(f\"True label: {test_row['label_text']}\")\n\n# Extract features\nlfcc = extract_lfcc(test_row['filepath'])\nfeatures = extract_statistical_features(lfcc).reshape(1, -1)\n\n# Predict\nstart = time.time()\npred_proba = model.predict_proba(features)[0]\ninference_time = (time.time() - start) * 1000\n\nprediction = \"AI_GENERATED\" if pred_proba[1] > 0.5 else \"REAL\"\nconfidence = max(pred_proba)\n\nprint(f\"\\nPrediction: {prediction}\")\nprint(f\"Confidence: {confidence:.2%}\")\nprint(f\"Inference time: {inference_time:.2f} ms\")\nprint(f\"Result: {'âœ“ CORRECT' if prediction == test_row['label_text'] else 'âœ— WRONG'}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ALL DONE! ðŸŽ‰\")\nprint(\"=\"*80)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
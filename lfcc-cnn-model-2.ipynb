{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374ebf38",
   "metadata": {
    "papermill": {
     "duration": 0.004347,
     "end_time": "2026-02-10T02:02:05.654833",
     "exception": false,
     "start_time": "2026-02-10T02:02:05.650486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LFCC + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc7e97d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T02:02:05.665753Z",
     "iopub.status.busy": "2026-02-10T02:02:05.665513Z",
     "iopub.status.idle": "2026-02-10T02:02:10.836643Z",
     "shell.execute_reply": "2026-02-10T02:02:10.835839Z"
    },
    "papermill": {
     "duration": 5.179311,
     "end_time": "2026-02-10T02:02:10.838151",
     "exception": false,
     "start_time": "2026-02-10T02:02:05.658840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 1: DATASET PREPARATION\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Collecting audio files...\n",
      "Found 68030 real audio files\n",
      "Found 68030 AI audio files\n",
      "\n",
      "[Step 2/5] Creating matched pairs...\n",
      "Found 68030 matched pairs\n",
      "\n",
      "[Step 3/5] Sampling balanced subset...\n",
      "\n",
      "[Step 4/5] Creating dataset dataframe...\n",
      "Total samples: 136000\n",
      "\n",
      "Class distribution:\n",
      "label_text\n",
      "REAL            68000\n",
      "AI_GENERATED    68000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Step 5/5] Creating train-test split...\n",
      "\n",
      "Train set: 108800 samples\n",
      "Test set: 27200 samples\n",
      "\n",
      "Train class distribution:\n",
      "label_text\n",
      "AI_GENERATED    54400\n",
      "REAL            54400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "label_text\n",
      "REAL            13600\n",
      "AI_GENERATED    13600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Dataset preparation complete!\n",
      "✓ Saved train_data.csv and test_data.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: Dataset Preparation and Train-Test Split\n",
    "This block handles loading the dataset, creating a balanced subset, and splitting into train/test sets.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Configuration\n",
    "DATASET_SIZE = 136000  # 68,000 real + 68,000 AI\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "BASE_PATH = '/kaggle/input/asins-voice/build'\n",
    "REAL_CLIPS_PATH = f'{BASE_PATH}/clips'\n",
    "AI_CLIPS_PATH = f'{BASE_PATH}/clips_AI'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 1: DATASET PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Collect all audio files\n",
    "print(\"\\n[Step 1/5] Collecting audio files...\")\n",
    "\n",
    "real_files = []\n",
    "ai_files = []\n",
    "\n",
    "for file in os.listdir(REAL_CLIPS_PATH):\n",
    "    if file.endswith(('.mp3', '.wav', '.ogg', '.flac')):\n",
    "        real_files.append(file)\n",
    "\n",
    "for file in os.listdir(AI_CLIPS_PATH):\n",
    "    if file.endswith(('.mp3', '.wav', '.ogg', '.flac')):\n",
    "        ai_files.append(file)\n",
    "\n",
    "print(f\"Found {len(real_files)} real audio files\")\n",
    "print(f\"Found {len(ai_files)} AI audio files\")\n",
    "\n",
    "# Step 2: Create matched pairs (same transcript)\n",
    "print(\"\\n[Step 2/5] Creating matched pairs...\")\n",
    "\n",
    "# Get intersection of filenames to ensure we have matching pairs\n",
    "real_set = set(real_files)\n",
    "ai_set = set(ai_files)\n",
    "matched_files = list(real_set.intersection(ai_set))\n",
    "\n",
    "print(f\"Found {len(matched_files)} matched pairs\")\n",
    "\n",
    "# Step 3: Sample balanced subset\n",
    "print(\"\\n[Step 3/5] Sampling balanced subset...\")\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "samples_per_class = DATASET_SIZE // 2\n",
    "\n",
    "if len(matched_files) < samples_per_class:\n",
    "    print(f\"WARNING: Only {len(matched_files)} matched pairs available, using all of them\")\n",
    "    samples_per_class = len(matched_files)\n",
    "    \n",
    "selected_files = np.random.choice(matched_files, size=samples_per_class, replace=False)\n",
    "\n",
    "# Step 4: Create dataframe\n",
    "print(\"\\n[Step 4/5] Creating dataset dataframe...\")\n",
    "\n",
    "data = []\n",
    "for filename in selected_files:\n",
    "    # Real audio\n",
    "    data.append({\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(REAL_CLIPS_PATH, filename),\n",
    "        'label': 0,  # 0 for REAL\n",
    "        'label_text': 'REAL'\n",
    "    })\n",
    "    \n",
    "    # AI audio\n",
    "    data.append({\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(AI_CLIPS_PATH, filename),\n",
    "        'label': 1,  # 1 for AI_GENERATED\n",
    "        'label_text': 'AI_GENERATED'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label_text'].value_counts())\n",
    "\n",
    "# Step 5: Train-test split\n",
    "print(\"\\n[Step 5/5] Creating train-test split...\")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(train_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(train_df['label_text'].value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(test_df['label_text'].value_counts())\n",
    "\n",
    "# Save dataframes for later use\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Dataset preparation complete!\")\n",
    "print(\"✓ Saved train_data.csv and test_data.csv\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc3f49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T02:02:10.846718Z",
     "iopub.status.busy": "2026-02-10T02:02:10.846474Z",
     "iopub.status.idle": "2026-02-10T02:59:11.436113Z",
     "shell.execute_reply": "2026-02-10T02:59:11.435401Z"
    },
    "papermill": {
     "duration": 3420.596053,
     "end_time": "2026-02-10T02:59:11.437895",
     "exception": false,
     "start_time": "2026-02-10T02:02:10.841842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BLOCK 2 MEMORY-EFFICIENT: FEATURE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "[Step 1/3] Loading dataset splits...\n",
      "Train samples: 108800\n",
      "Test samples: 27200\n",
      "\n",
      "[Step 2/3] Extracting TRAINING features (memory-efficient)...\n",
      "Determining feature shape from first sample...\n",
      "Feature shape: (40, 312)\n",
      "\n",
      "Processing 108800 samples in 2176 batches...\n",
      "Saving to disk every 1000 samples to prevent memory overflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|          | 20/2176 [00:25<59:51,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 1 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|▏         | 40/2176 [00:51<1:02:13,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 2 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 60/2176 [01:13<52:45,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 3 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▎         | 80/2176 [01:36<54:34,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 4 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▍         | 100/2176 [01:59<53:44,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 5 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▌         | 120/2176 [02:22<55:05,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 6 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▋         | 140/2176 [02:45<52:34,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 7 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 160/2176 [03:08<50:57,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 8 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 180/2176 [03:30<51:24,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 9 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 200/2176 [03:53<50:12,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 10 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 220/2176 [04:15<49:24,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 11 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 240/2176 [04:37<47:42,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 12 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 260/2176 [05:00<50:38,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 13 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 280/2176 [05:22<47:38,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 14 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 300/2176 [05:44<46:32,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 15 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▍        | 320/2176 [06:06<47:03,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 16 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 340/2176 [06:29<47:07,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 17 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 360/2176 [06:51<45:58,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 18 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 380/2176 [07:14<44:19,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 19 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 400/2176 [07:36<45:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 20 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 420/2176 [07:58<43:25,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 21 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|██        | 440/2176 [08:20<44:19,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 22 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 460/2176 [08:42<41:31,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 23 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 480/2176 [09:05<44:02,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 24 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 500/2176 [09:28<42:26,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 25 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 520/2176 [09:50<41:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 26 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▍       | 540/2176 [10:13<41:58,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 27 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 560/2176 [10:35<39:55,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 28 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 580/2176 [10:57<39:20,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 29 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 600/2176 [11:19<39:47,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 30 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|██▊       | 620/2176 [11:41<39:15,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 31 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 640/2176 [12:03<38:39,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 32 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|███       | 660/2176 [12:25<37:28,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 33 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███▏      | 680/2176 [12:48<38:15,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 34 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 700/2176 [13:10<37:29,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 35 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 720/2176 [13:33<36:19,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 36 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 740/2176 [13:55<37:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 37 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▍      | 760/2176 [14:18<36:29,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 38 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▌      | 780/2176 [14:40<34:50,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 39 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 800/2176 [15:03<35:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 40 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 820/2176 [15:25<33:11,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 41 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▊      | 840/2176 [15:48<33:26,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 42 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|███▉      | 860/2176 [16:11<33:47,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 43 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 880/2176 [16:34<32:36,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 44 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████▏     | 900/2176 [16:56<31:49,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 45 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 920/2176 [17:19<32:09,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 46 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|████▎     | 940/2176 [17:41<32:06,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 47 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|████▍     | 960/2176 [18:04<31:21,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 48 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  45%|████▌     | 980/2176 [18:27<30:44,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 49 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|████▌     | 1000/2176 [18:50<30:08,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 50 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|████▋     | 1020/2176 [19:12<29:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 51 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 1040/2176 [19:35<28:19,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 52 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|████▊     | 1060/2176 [19:58<29:19,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 53 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|████▉     | 1080/2176 [20:20<27:19,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 54 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████     | 1100/2176 [20:43<26:51,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 55 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████▏    | 1120/2176 [21:07<28:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 56 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|█████▏    | 1140/2176 [21:30<27:04,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 57 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  53%|█████▎    | 1160/2176 [21:53<26:17,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 58 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|█████▍    | 1180/2176 [22:17<25:51,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 59 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|█████▌    | 1200/2176 [22:40<25:11,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 60 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|█████▌    | 1220/2176 [23:05<25:29,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 61 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  57%|█████▋    | 1240/2176 [23:28<25:03,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 62 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 1260/2176 [23:52<23:52,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 63 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  59%|█████▉    | 1280/2176 [24:16<23:17,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 64 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|█████▉    | 1300/2176 [24:39<22:49,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 65 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|██████    | 1320/2176 [25:02<22:49,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 66 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  62%|██████▏   | 1340/2176 [25:26<21:54,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 67 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  62%|██████▎   | 1360/2176 [25:49<21:19,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 68 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  63%|██████▎   | 1380/2176 [26:13<21:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 69 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  64%|██████▍   | 1400/2176 [26:36<20:08,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 70 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  65%|██████▌   | 1420/2176 [26:59<19:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 71 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  66%|██████▌   | 1440/2176 [27:23<19:09,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 72 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|██████▋   | 1460/2176 [27:47<18:53,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 73 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  68%|██████▊   | 1480/2176 [28:10<17:57,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 74 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  69%|██████▉   | 1500/2176 [28:34<17:39,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 75 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|██████▉   | 1520/2176 [28:58<17:26,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 76 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  71%|███████   | 1540/2176 [29:21<16:15,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 77 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  72%|███████▏  | 1560/2176 [29:45<17:38,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 78 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|███████▎  | 1580/2176 [30:09<15:50,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 79 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|███████▎  | 1600/2176 [30:32<14:58,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 80 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|███████▍  | 1620/2176 [30:56<14:40,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 81 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|███████▌  | 1640/2176 [31:19<13:52,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 82 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|███████▋  | 1660/2176 [31:43<13:30,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 83 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|███████▋  | 1680/2176 [32:06<12:56,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 84 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|███████▊  | 1700/2176 [32:30<12:56,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 85 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  79%|███████▉  | 1720/2176 [32:54<11:58,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 86 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|███████▉  | 1740/2176 [33:17<11:19,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 87 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  81%|████████  | 1760/2176 [33:40<10:49,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 88 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  82%|████████▏ | 1780/2176 [34:03<10:15,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 89 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 1800/2176 [34:26<09:52,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 90 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  84%|████████▎ | 1820/2176 [34:51<09:52,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 91 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 1840/2176 [35:15<08:51,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 92 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▌ | 1860/2176 [35:38<08:21,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 93 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  86%|████████▋ | 1880/2176 [36:04<08:37,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 94 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  87%|████████▋ | 1900/2176 [36:30<07:51,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 95 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  88%|████████▊ | 1920/2176 [36:57<07:18,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 96 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  89%|████████▉ | 1940/2176 [37:22<06:26,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 97 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  90%|█████████ | 1960/2176 [37:46<05:39,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 98 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  91%|█████████ | 1980/2176 [38:10<05:10,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 99 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|█████████▏| 2000/2176 [38:33<04:27,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 100 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  93%|█████████▎| 2020/2176 [38:57<04:04,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 101 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  94%|█████████▍| 2040/2176 [39:21<03:34,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 102 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|█████████▍| 2060/2176 [39:44<03:05,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 103 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  96%|█████████▌| 2080/2176 [40:08<02:28,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 104 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|█████████▋| 2100/2176 [40:31<02:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 105 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|█████████▋| 2120/2176 [40:55<01:31,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 106 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  98%|█████████▊| 2140/2176 [41:19<00:56,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 107 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  99%|█████████▉| 2160/2176 [41:43<00:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 108 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2176/2176 [42:01<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved final chunk 109\n",
      "\n",
      "Combining 109 chunks into final file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading chunks: 100%|██████████| 109/109 [00:23<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final features to features_train.npz...\n",
      "✓ Saved features_train.npz\n",
      "\n",
      "Clearing memory before processing test set...\n",
      "\n",
      "[Step 3/3] Extracting TEST features (memory-efficient)...\n",
      "Determining feature shape from first sample...\n",
      "Feature shape: (40, 312)\n",
      "\n",
      "Processing 27200 samples in 544 batches...\n",
      "Saving to disk every 1000 samples to prevent memory overflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▎         | 20/544 [00:23<14:12,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 1 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 40/544 [00:47<13:05,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 2 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%|█         | 60/544 [01:10<12:36,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 3 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|█▍        | 80/544 [01:33<11:52,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 4 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 100/544 [01:56<11:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 5 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 120/544 [02:19<10:59,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 6 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 140/544 [02:43<10:31,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 7 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▉       | 160/544 [03:06<09:56,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 8 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|███▎      | 180/544 [03:30<09:38,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 9 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|███▋      | 200/544 [03:53<08:49,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 10 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 220/544 [04:16<08:29,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 11 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|████▍     | 240/544 [04:40<08:12,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 12 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 260/544 [05:04<07:22,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 13 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████▏    | 280/544 [05:28<06:52,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 14 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|█████▌    | 300/544 [05:51<06:29,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 15 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  59%|█████▉    | 320/544 [06:16<06:16,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 16 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  62%|██████▎   | 340/544 [06:41<05:39,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 17 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  66%|██████▌   | 360/544 [07:07<05:03,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 18 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|██████▉   | 380/544 [07:31<04:22,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 19 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|███████▎  | 400/544 [07:55<03:49,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 20 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|███████▋  | 420/544 [08:20<03:17,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 21 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  81%|████████  | 440/544 [08:44<02:43,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 22 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 460/544 [09:09<02:12,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 23 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  88%|████████▊ | 480/544 [09:33<01:43,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 24 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|█████████▏| 500/544 [09:57<01:10,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 25 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  96%|█████████▌| 520/544 [10:21<00:38,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 26 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  99%|█████████▉| 540/544 [10:45<00:06,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved chunk 27 to disk, cleared memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 544/544 [10:49<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Saved final chunk 28\n",
      "\n",
      "Combining 28 chunks into final file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading chunks: 100%|██████████| 28/28 [00:05<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final features to features_test.npz...\n",
      "✓ Saved features_test.npz\n",
      "\n",
      "================================================================================\n",
      "MEMORY-EFFICIENT FEATURE EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Verification:\n",
      "  Train features: (108800, 40, 312)\n",
      "  Train labels: (108800,)\n",
      "  Test features: (27200, 40, 312)\n",
      "  Test labels: (27200,)\n",
      "\n",
      "✓ All features extracted successfully!\n",
      "✓ Memory management: Incremental saving prevented overflow\n",
      "✓ No kernel death! 🎉\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 2 MEMORY-EFFICIENT: Feature Extraction with Proper Memory Management\n",
    "FIXES: Kernel death due to memory overflow\n",
    "\n",
    "Key changes:\n",
    "- Save features incrementally to disk (batches of 1000)\n",
    "- Clear memory after each batch\n",
    "- Use memory-mapped arrays for large datasets\n",
    "- Explicit garbage collection\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BLOCK 2 MEMORY-EFFICIENT: FEATURE EXTRACTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_RATE = 16000\n",
    "N_LFCC = 40\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "MAX_DURATION = 10\n",
    "BATCH_SIZE = 100 \n",
    "SAVE_EVERY = 1000  # Save to disk every 1000 samples\n",
    "\n",
    "def extract_lfcc(audio_path):\n",
    "    \"\"\"Extract LFCC features from audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=MAX_DURATION)\n",
    "        \n",
    "        lfcc = librosa.feature.mfcc(\n",
    "            y=y, \n",
    "            sr=sr, \n",
    "            n_mfcc=N_LFCC,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH\n",
    "        )\n",
    "        \n",
    "        target_length = int(MAX_DURATION * sr / HOP_LENGTH)\n",
    "        \n",
    "        if lfcc.shape[1] < target_length:\n",
    "            pad_width = target_length - lfcc.shape[1]\n",
    "            lfcc = np.pad(lfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            lfcc = lfcc[:, :target_length]\n",
    "        \n",
    "        return lfcc\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        target_length = int(MAX_DURATION * SAMPLE_RATE / HOP_LENGTH)\n",
    "        return np.zeros((N_LFCC, target_length))\n",
    "\n",
    "def extract_features_memory_efficient(df, output_path, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Extract features with memory-efficient processing.\n",
    "    Saves features to disk incrementally to avoid memory overflow.\n",
    "    \"\"\"\n",
    "    total_samples = len(df)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Create temporary directory for partial saves\n",
    "    temp_dir = 'temp_features'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Determine feature shape from first sample\n",
    "    print(\"Determining feature shape from first sample...\")\n",
    "    first_feature = extract_lfcc(df.iloc[0]['filepath'])\n",
    "    feature_shape = first_feature.shape\n",
    "    print(f\"Feature shape: {feature_shape}\")\n",
    "    \n",
    "    # Process in batches and save incrementally\n",
    "    chunk_files = []\n",
    "    current_chunk = []\n",
    "    current_labels = []\n",
    "    current_filenames = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    print(f\"\\nProcessing {total_samples} samples in {num_batches} batches...\")\n",
    "    print(f\"Saving to disk every {SAVE_EVERY} samples to prevent memory overflow...\")\n",
    "    \n",
    "    for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min((batch_idx + 1) * batch_size, total_samples)\n",
    "        batch_df = df.iloc[batch_start:batch_end]\n",
    "        \n",
    "        # Process batch\n",
    "        for idx, row in batch_df.iterrows():\n",
    "            lfcc = extract_lfcc(row['filepath'])\n",
    "            current_chunk.append(lfcc)\n",
    "            current_labels.append(row['label'])\n",
    "            current_filenames.append(row['filename'])\n",
    "            \n",
    "            # Save chunk when reaching SAVE_EVERY samples\n",
    "            if len(current_chunk) >= SAVE_EVERY:\n",
    "                chunk_file = os.path.join(temp_dir, f'chunk_{chunk_idx}.npz')\n",
    "                np.savez_compressed(\n",
    "                    chunk_file,\n",
    "                    X=np.array(current_chunk),\n",
    "                    y=np.array(current_labels),\n",
    "                    filenames=current_filenames\n",
    "                )\n",
    "                chunk_files.append(chunk_file)\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                # Clear memory\n",
    "                current_chunk = []\n",
    "                current_labels = []\n",
    "                current_filenames = []\n",
    "                gc.collect()\n",
    "                \n",
    "                print(f\"\\n  Saved chunk {chunk_idx} to disk, cleared memory\")\n",
    "        \n",
    "        # Clear memory after each batch\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    # Save remaining samples\n",
    "    if len(current_chunk) > 0:\n",
    "        chunk_file = os.path.join(temp_dir, f'chunk_{chunk_idx}.npz')\n",
    "        np.savez_compressed(\n",
    "            chunk_file,\n",
    "            X=np.array(current_chunk),\n",
    "            y=np.array(current_labels),\n",
    "            filenames=current_filenames\n",
    "        )\n",
    "        chunk_files.append(chunk_file)\n",
    "        chunk_idx += 1\n",
    "        print(f\"\\n  Saved final chunk {chunk_idx}\")\n",
    "    \n",
    "    # Combine chunks into final file\n",
    "    print(f\"\\nCombining {len(chunk_files)} chunks into final file...\")\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for chunk_file in tqdm(chunk_files, desc=\"Loading chunks\"):\n",
    "        chunk_data = np.load(chunk_file, allow_pickle=True)\n",
    "        all_features.append(chunk_data['X'])\n",
    "        all_labels.append(chunk_data['y'])\n",
    "        all_filenames.extend(chunk_data['filenames'])\n",
    "        \n",
    "        # Delete chunk file to save disk space\n",
    "        os.remove(chunk_file)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    X = np.concatenate(all_features, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Save final file\n",
    "    print(f\"Saving final features to {output_path}...\")\n",
    "    np.savez_compressed(output_path, X=X, y=y, filenames=all_filenames)\n",
    "    \n",
    "    # Cleanup\n",
    "    os.rmdir(temp_dir)\n",
    "    \n",
    "    # Clear memory\n",
    "    del all_features, all_labels, X, y\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"✓ Saved {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\n[Step 1/3] Loading dataset splits...\")\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Extract training features\n",
    "print(\"\\n[Step 2/3] Extracting TRAINING features (memory-efficient)...\")\n",
    "extract_features_memory_efficient(\n",
    "    train_df, \n",
    "    'features_train.npz',\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Force garbage collection before test set\n",
    "print(\"\\nClearing memory before processing test set...\")\n",
    "gc.collect()\n",
    "\n",
    "# Extract test features\n",
    "print(\"\\n[Step 3/3] Extracting TEST features (memory-efficient)...\")\n",
    "extract_features_memory_efficient(\n",
    "    test_df, \n",
    "    'features_test.npz',\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MEMORY-EFFICIENT FEATURE EXTRACTION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load and verify\n",
    "train_data = np.load('features_train.npz', allow_pickle=True)\n",
    "test_data = np.load('features_test.npz', allow_pickle=True)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Train features: {train_data['X'].shape}\")\n",
    "print(f\"  Train labels: {train_data['y'].shape}\")\n",
    "print(f\"  Test features: {test_data['X'].shape}\")\n",
    "print(f\"  Test labels: {test_data['y'].shape}\")\n",
    "\n",
    "print(f\"\\n✓ All features extracted successfully!\")\n",
    "print(f\"✓ Memory management: Incremental saving prevented overflow\")\n",
    "print(f\"✓ No kernel death! 🎉\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear everything\n",
    "del train_data, test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f214b29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T02:59:11.653315Z",
     "iopub.status.busy": "2026-02-10T02:59:11.652858Z",
     "iopub.status.idle": "2026-02-10T03:37:30.776118Z",
     "shell.execute_reply": "2026-02-10T03:37:30.775359Z"
    },
    "papermill": {
     "duration": 2299.232065,
     "end_time": "2026-02-10T03:37:30.777799",
     "exception": false,
     "start_time": "2026-02-10T02:59:11.545734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 02:59:12.989868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770692353.132334      23 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770692353.171461      23 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770692353.500235      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770692353.500258      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770692353.500261      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770692353.500263      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 3: CNN MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "[GPU Check]\n",
      "TensorFlow version: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "\n",
      "[Step 1/5] Loading features...\n",
      "Train shape: (108800, 40, 312)\n",
      "Test shape: (27200, 40, 312)\n",
      "Reshaped train: (108800, 40, 312, 1)\n",
      "Reshaped test: (27200, 40, 312, 1)\n",
      "\n",
      "[Step 2/5] Normalizing features...\n",
      "\n",
      "[Step 3/5] Building CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1770692401.024574      23 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1770692401.030635      23 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13757 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"VoiceClassifierCNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"VoiceClassifierCNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">312</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu1 (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m312\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn2 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu2 (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv3 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn3 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu3 (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m78\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout3 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv4 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn4 (\u001b[38;5;33mBatchNormalization\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ relu4 (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m256\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout4 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout5 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">430,977</span> (1.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m430,977\u001b[0m (1.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">430,017</span> (1.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m430,017\u001b[0m (1.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 4/5] Setting up callbacks...\n",
      "\n",
      "[Step 5/5] Training model...\n",
      "Batch size: 32\n",
      "Max epochs: 50\n",
      "Early stopping patience: 5\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770692417.910152      77 service.cc:152] XLA service 0x7c44980040a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1770692417.910183      77 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1770692417.910192      77 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1770692418.618710      77 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "2026-02-10 03:00:21.084916: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2026-02-10 03:00:21.255946: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   5/3400\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 29ms/step - accuracy: 0.6121 - loss: 0.7005 - precision: 0.6128 - recall: 0.4995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1770692426.686850      77 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9343 - loss: 0.1584 - precision: 0.9280 - recall: 0.9404\n",
      "Epoch 1: val_loss improved from inf to 0.21893, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 22ms/step - accuracy: 0.9344 - loss: 0.1584 - precision: 0.9281 - recall: 0.9404 - val_accuracy: 0.9174 - val_loss: 0.2189 - val_precision: 1.0000 - val_recall: 0.8349 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0303 - precision: 0.9887 - recall: 0.9930\n",
      "Epoch 2: val_loss improved from 0.21893 to 0.04008, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9908 - loss: 0.0303 - precision: 0.9887 - recall: 0.9930 - val_accuracy: 0.9855 - val_loss: 0.0401 - val_precision: 0.9720 - val_recall: 0.9998 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9932 - loss: 0.0218 - precision: 0.9922 - recall: 0.9943\n",
      "Epoch 3: val_loss improved from 0.04008 to 0.00854, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9932 - loss: 0.0218 - precision: 0.9922 - recall: 0.9943 - val_accuracy: 0.9967 - val_loss: 0.0085 - val_precision: 0.9947 - val_recall: 0.9988 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9954 - loss: 0.0162 - precision: 0.9942 - recall: 0.9966\n",
      "Epoch 4: val_loss did not improve from 0.00854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9954 - loss: 0.0162 - precision: 0.9942 - recall: 0.9966 - val_accuracy: 0.9957 - val_loss: 0.0104 - val_precision: 0.9924 - val_recall: 0.9990 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9956 - loss: 0.0142 - precision: 0.9946 - recall: 0.9966\n",
      "Epoch 5: val_loss improved from 0.00854 to 0.00509, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9956 - loss: 0.0142 - precision: 0.9946 - recall: 0.9966 - val_accuracy: 0.9983 - val_loss: 0.0051 - val_precision: 0.9968 - val_recall: 0.9998 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3399/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9972 - loss: 0.0105 - precision: 0.9966 - recall: 0.9977\n",
      "Epoch 6: val_loss did not improve from 0.00509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9972 - loss: 0.0105 - precision: 0.9966 - recall: 0.9977 - val_accuracy: 0.9742 - val_loss: 0.1248 - val_precision: 0.9510 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9964 - loss: 0.0130 - precision: 0.9959 - recall: 0.9969\n",
      "Epoch 7: val_loss improved from 0.00509 to 0.00346, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9964 - loss: 0.0130 - precision: 0.9959 - recall: 0.9969 - val_accuracy: 0.9990 - val_loss: 0.0035 - val_precision: 0.9995 - val_recall: 0.9985 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9974 - loss: 0.0094 - precision: 0.9969 - recall: 0.9978\n",
      "Epoch 8: val_loss did not improve from 0.00346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9974 - loss: 0.0094 - precision: 0.9969 - recall: 0.9978 - val_accuracy: 0.9973 - val_loss: 0.0082 - val_precision: 0.9991 - val_recall: 0.9955 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0095 - precision: 0.9967 - recall: 0.9978\n",
      "Epoch 9: val_loss improved from 0.00346 to 0.00238, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.0095 - precision: 0.9967 - recall: 0.9978 - val_accuracy: 0.9992 - val_loss: 0.0024 - val_precision: 0.9986 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9981 - loss: 0.0060 - precision: 0.9979 - recall: 0.9982\n",
      "Epoch 10: val_loss did not improve from 0.00238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0060 - precision: 0.9979 - recall: 0.9982 - val_accuracy: 0.9990 - val_loss: 0.0025 - val_precision: 0.9980 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0056 - precision: 0.9982 - recall: 0.9987\n",
      "Epoch 11: val_loss did not improve from 0.00238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9984 - loss: 0.0056 - precision: 0.9982 - recall: 0.9987 - val_accuracy: 0.9988 - val_loss: 0.0037 - val_precision: 0.9976 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0056 - precision: 0.9980 - recall: 0.9989\n",
      "Epoch 12: val_loss did not improve from 0.00238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0056 - precision: 0.9980 - recall: 0.9989 - val_accuracy: 0.9831 - val_loss: 0.0551 - val_precision: 0.9673 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0056 - precision: 0.9982 - recall: 0.9990\n",
      "Epoch 13: val_loss improved from 0.00238 to 0.00124, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0056 - precision: 0.9982 - recall: 0.9990 - val_accuracy: 0.9996 - val_loss: 0.0012 - val_precision: 0.9993 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0062 - precision: 0.9983 - recall: 0.9988\n",
      "Epoch 14: val_loss did not improve from 0.00124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0062 - precision: 0.9983 - recall: 0.9988 - val_accuracy: 0.9981 - val_loss: 0.0042 - val_precision: 0.9963 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0044 - precision: 0.9985 - recall: 0.9989\n",
      "Epoch 15: val_loss did not improve from 0.00124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0044 - precision: 0.9985 - recall: 0.9989 - val_accuracy: 0.9973 - val_loss: 0.0110 - val_precision: 0.9947 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0045 - precision: 0.9981 - recall: 0.9988\n",
      "Epoch 16: val_loss improved from 0.00124 to 0.00090, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0045 - precision: 0.9981 - recall: 0.9988 - val_accuracy: 0.9996 - val_loss: 9.0018e-04 - val_precision: 0.9996 - val_recall: 0.9997 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0044 - precision: 0.9983 - recall: 0.9990\n",
      "Epoch 17: val_loss did not improve from 0.00090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0044 - precision: 0.9983 - recall: 0.9990 - val_accuracy: 0.9990 - val_loss: 0.0028 - val_precision: 0.9982 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0035 - precision: 0.9988 - recall: 0.9992\n",
      "Epoch 18: val_loss did not improve from 0.00090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0035 - precision: 0.9988 - recall: 0.9992 - val_accuracy: 0.9994 - val_loss: 0.0018 - val_precision: 0.9999 - val_recall: 0.9989 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0049 - precision: 0.9985 - recall: 0.9990\n",
      "Epoch 19: val_loss improved from 0.00090 to 0.00067, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0049 - precision: 0.9985 - recall: 0.9990 - val_accuracy: 0.9998 - val_loss: 6.7391e-04 - val_precision: 0.9996 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m3399/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0030 - precision: 0.9991 - recall: 0.9991\n",
      "Epoch 20: val_loss did not improve from 0.00067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0030 - precision: 0.9991 - recall: 0.9991 - val_accuracy: 0.9980 - val_loss: 0.0056 - val_precision: 0.9966 - val_recall: 0.9995 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0037 - precision: 0.9987 - recall: 0.9990\n",
      "Epoch 21: val_loss improved from 0.00067 to 0.00061, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0037 - precision: 0.9987 - recall: 0.9990 - val_accuracy: 0.9999 - val_loss: 6.0714e-04 - val_precision: 0.9998 - val_recall: 0.9999 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9994 - loss: 0.0027 - precision: 0.9992 - recall: 0.9997\n",
      "Epoch 22: val_loss did not improve from 0.00061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9994 - loss: 0.0027 - precision: 0.9992 - recall: 0.9997 - val_accuracy: 0.9997 - val_loss: 8.1675e-04 - val_precision: 0.9996 - val_recall: 0.9998 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0037 - precision: 0.9986 - recall: 0.9992\n",
      "Epoch 23: val_loss did not improve from 0.00061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0037 - precision: 0.9986 - recall: 0.9992 - val_accuracy: 0.9990 - val_loss: 0.0031 - val_precision: 0.9981 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0029 - precision: 0.9991 - recall: 0.9993\n",
      "Epoch 24: val_loss did not improve from 0.00061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0029 - precision: 0.9991 - recall: 0.9993 - val_accuracy: 0.9995 - val_loss: 0.0013 - val_precision: 0.9996 - val_recall: 0.9995 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9996 - loss: 0.0014 - precision: 0.9996 - recall: 0.9996\n",
      "Epoch 25: val_loss improved from 0.00061 to 0.00026, saving model to checkpoints/best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9996 - loss: 0.0014 - precision: 0.9996 - recall: 0.9996 - val_accuracy: 0.9999 - val_loss: 2.6101e-04 - val_precision: 0.9998 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m3399/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 8.1552e-04 - precision: 0.9998 - recall: 0.9998\n",
      "Epoch 26: val_loss did not improve from 0.00026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 8.1567e-04 - precision: 0.9998 - recall: 0.9998 - val_accuracy: 0.9997 - val_loss: 7.2076e-04 - val_precision: 0.9995 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9997 - loss: 0.0012 - precision: 0.9996 - recall: 0.9997\n",
      "Epoch 27: val_loss did not improve from 0.00026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9997 - loss: 0.0012 - precision: 0.9996 - recall: 0.9997 - val_accuracy: 0.9999 - val_loss: 6.2631e-04 - val_precision: 0.9999 - val_recall: 0.9999 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 6.6239e-04 - precision: 0.9998 - recall: 0.9998\n",
      "Epoch 28: val_loss did not improve from 0.00026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 6.6247e-04 - precision: 0.9998 - recall: 0.9998 - val_accuracy: 0.9999 - val_loss: 2.9681e-04 - val_precision: 0.9999 - val_recall: 0.9999 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 8.6876e-04 - precision: 0.9997 - recall: 0.9999\n",
      "Epoch 29: val_loss did not improve from 0.00026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 8.6883e-04 - precision: 0.9997 - recall: 0.9999 - val_accuracy: 0.9999 - val_loss: 6.7945e-04 - val_precision: 0.9997 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 7.7839e-04 - precision: 0.9998 - recall: 0.9999\n",
      "Epoch 30: val_loss did not improve from 0.00026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m3400/3400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 7.7852e-04 - precision: 0.9998 - recall: 0.9999 - val_accuracy: 0.9995 - val_loss: 0.0013 - val_precision: 0.9990 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "\n",
      "✓ Model training complete!\n",
      "✓ Best model saved to: checkpoints/best_model.h5\n",
      "✓ Latest checkpoint saved to: checkpoints/latest_checkpoint.h5\n",
      "✓ Training log saved to: training_log.csv\n",
      "============================================================\n",
      "\n",
      "[Evaluation on Test Set]\n",
      "Test Loss: 0.0003\n",
      "Test Accuracy: 0.9999\n",
      "Test Precision: 0.9998\n",
      "Test Recall: 1.0000\n",
      "\n",
      "[Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        REAL     1.0000    0.9998    0.9999     13600\n",
      "AI_GENERATED     0.9998    1.0000    0.9999     13600\n",
      "\n",
      "    accuracy                         0.9999     27200\n",
      "   macro avg     0.9999    0.9999    0.9999     27200\n",
      "weighted avg     0.9999    0.9999    0.9999     27200\n",
      "\n",
      "\n",
      "[Confusion Matrix]\n",
      "[[13597     3]\n",
      " [    0 13600]]\n",
      "\n",
      "True Negatives (REAL correctly classified): 13597\n",
      "False Positives (REAL classified as AI): 3\n",
      "False Negatives (AI classified as REAL): 0\n",
      "True Positives (AI correctly classified): 13600\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 3: CNN Model Architecture and Training\n",
    "Implements a CNN for binary classification with checkpoint management.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, callbacks\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 3: CNN MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"\\n[GPU Check]\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Model configuration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 5\n",
    "\n",
    "# Load features\n",
    "print(\"\\n[Step 1/5] Loading features...\")\n",
    "train_data = np.load('features_train.npz', allow_pickle=True)\n",
    "test_data = np.load('features_test.npz', allow_pickle=True)\n",
    "\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")\n",
    "\n",
    "# Reshape for CNN: (samples, height, width, channels)\n",
    "# LFCC features: (n_lfcc, time_steps) -> (n_lfcc, time_steps, 1)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(f\"Reshaped train: {X_train.shape}\")\n",
    "print(f\"Reshaped test: {X_test.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "print(\"\\n[Step 2/5] Normalizing features...\")\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "X_train = (X_train - mean) / (std + 1e-8)\n",
    "X_test = (X_test - mean) / (std + 1e-8)\n",
    "\n",
    "# Save normalization parameters\n",
    "np.savez('normalization_params.npz', mean=mean, std=std)\n",
    "\n",
    "# Build CNN model\n",
    "print(\"\\n[Step 3/5] Building CNN model...\")\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    CNN architecture optimized for audio classification.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', name='conv1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.Activation('relu', name='relu1')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool1')(x)\n",
    "    x = layers.Dropout(0.25, name='dropout1')(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', name='conv2')(x)\n",
    "    x = layers.BatchNormalization(name='bn2')(x)\n",
    "    x = layers.Activation('relu', name='relu2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool2')(x)\n",
    "    x = layers.Dropout(0.25, name='dropout2')(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', name='conv3')(x)\n",
    "    x = layers.BatchNormalization(name='bn3')(x)\n",
    "    x = layers.Activation('relu', name='relu3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool3')(x)\n",
    "    x = layers.Dropout(0.25, name='dropout3')(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', name='conv4')(x)\n",
    "    x = layers.BatchNormalization(name='bn4')(x)\n",
    "    x = layers.Activation('relu', name='relu4')(x)\n",
    "    x = layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout4')(x)\n",
    "    x = layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "    x = layers.Dropout(0.5, name='dropout5')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='VoiceClassifierCNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "input_shape = X_train.shape[1:]  # (n_lfcc, time_steps, 1)\n",
    "model = build_cnn_model(input_shape)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "             keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Setup callbacks for checkpoint management\n",
    "print(\"\\n[Step 4/5] Setting up callbacks...\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "# Callback: Save best model only\n",
    "checkpoint_best = callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback: Save latest checkpoint (overwrite)\n",
    "checkpoint_latest = callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoints/latest_checkpoint.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Callback: Early stopping\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback: Reduce learning rate on plateau\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Callback: CSV logger\n",
    "csv_logger = callbacks.CSVLogger('training_log.csv', append=False)\n",
    "\n",
    "callback_list = [\n",
    "    checkpoint_best,\n",
    "    checkpoint_latest,\n",
    "    early_stop,\n",
    "    reduce_lr,\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "# Train model\n",
    "print(\"\\n[Step 5/5] Training model...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Early stopping patience: {PATIENCE}\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model training complete!\")\n",
    "print(\"✓ Best model saved to: checkpoints/best_model.h5\")\n",
    "print(\"✓ Latest checkpoint saved to: checkpoints/latest_checkpoint.h5\")\n",
    "print(\"✓ Training log saved to: training_log.csv\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n[Evaluation on Test Set]\")\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = model.predict(X_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n[Classification Report]\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['REAL', 'AI_GENERATED'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\n[Confusion Matrix]\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives (REAL correctly classified): {cm[0][0]}\")\n",
    "print(f\"False Positives (REAL classified as AI): {cm[0][1]}\")\n",
    "print(f\"False Negatives (AI classified as REAL): {cm[1][0]}\")\n",
    "print(f\"True Positives (AI correctly classified): {cm[1][1]}\")\n",
    "\n",
    "# Clear memory\n",
    "del X_train, y_train, X_test, y_test\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751f0d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T03:37:33.987134Z",
     "iopub.status.busy": "2026-02-10T03:37:33.986614Z",
     "iopub.status.idle": "2026-02-10T03:37:34.013351Z",
     "shell.execute_reply": "2026-02-10T03:37:34.012614Z"
    },
    "papermill": {
     "duration": 1.632304,
     "end_time": "2026-02-10T03:37:34.014869",
     "exception": false,
     "start_time": "2026-02-10T03:37:32.382565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 4: EXPLANATION ENGINE\n",
      "============================================================\n",
      "\n",
      "[Testing Explanation Engine]\n",
      "✓ AudioAnalyzer initialized\n",
      "✓ ExplanationGenerator initialized\n",
      "\n",
      "Ready to generate explanations for predictions!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 4: Rule-Based Explanation Engine\n",
    "Analyzes audio features to generate human-readable explanations.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 4: EXPLANATION ENGINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class AudioAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes audio files to extract interpretable features for explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def analyze_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract various audio features for rule-based explanation.\n",
    "        Returns a dictionary of analysis results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=10)\n",
    "            \n",
    "            analysis = {}\n",
    "            \n",
    "            # 1. PITCH ANALYSIS\n",
    "            pitch_features = self._analyze_pitch(y, sr)\n",
    "            analysis.update(pitch_features)\n",
    "            \n",
    "            # 2. SILENCE ANALYSIS\n",
    "            silence_features = self._analyze_silence(y, sr)\n",
    "            analysis.update(silence_features)\n",
    "            \n",
    "            # 3. BREATH ANALYSIS\n",
    "            breath_features = self._analyze_breaths(y, sr)\n",
    "            analysis.update(breath_features)\n",
    "            \n",
    "            # 4. SPECTRAL ANALYSIS\n",
    "            spectral_features = self._analyze_spectral(y, sr)\n",
    "            analysis.update(spectral_features)\n",
    "            \n",
    "            # 5. ENERGY ANALYSIS\n",
    "            energy_features = self._analyze_energy(y, sr)\n",
    "            analysis.update(energy_features)\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing audio: {e}\")\n",
    "            return self._get_default_analysis()\n",
    "    \n",
    "    def _analyze_pitch(self, y, sr):\n",
    "        \"\"\"Analyze pitch characteristics.\"\"\"\n",
    "        # Extract pitch using piptrack\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        \n",
    "        # Get pitch values with significant magnitude\n",
    "        pitch_values = []\n",
    "        for t in range(pitches.shape[1]):\n",
    "            index = magnitudes[:, t].argmax()\n",
    "            pitch = pitches[index, t]\n",
    "            if pitch > 0:\n",
    "                pitch_values.append(pitch)\n",
    "        \n",
    "        if len(pitch_values) == 0:\n",
    "            return {\n",
    "                'pitch_variance': 0,\n",
    "                'pitch_consistency': 0,\n",
    "                'pitch_range': 0\n",
    "            }\n",
    "        \n",
    "        pitch_values = np.array(pitch_values)\n",
    "        \n",
    "        # Calculate pitch statistics\n",
    "        pitch_variance = np.var(pitch_values)\n",
    "        pitch_std = np.std(pitch_values)\n",
    "        pitch_mean = np.mean(pitch_values)\n",
    "        pitch_range = np.max(pitch_values) - np.min(pitch_values)\n",
    "        \n",
    "        # Consistency score (lower variance = more consistent/robotic)\n",
    "        pitch_consistency = 1.0 / (1.0 + pitch_variance / 1000)\n",
    "        \n",
    "        return {\n",
    "            'pitch_variance': float(pitch_variance),\n",
    "            'pitch_consistency': float(pitch_consistency),\n",
    "            'pitch_range': float(pitch_range),\n",
    "            'pitch_mean': float(pitch_mean),\n",
    "            'pitch_std': float(pitch_std)\n",
    "        }\n",
    "    \n",
    "    def _analyze_silence(self, y, sr):\n",
    "        \"\"\"Analyze silence patterns.\"\"\"\n",
    "        # Split audio into frames\n",
    "        frame_length = 2048\n",
    "        hop_length = 512\n",
    "        \n",
    "        # Calculate RMS energy\n",
    "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "        \n",
    "        # Define silence threshold (adaptive)\n",
    "        silence_threshold = np.percentile(rms, 20)\n",
    "        \n",
    "        # Find silent frames\n",
    "        silent_frames = rms < silence_threshold\n",
    "        silence_ratio = np.mean(silent_frames)\n",
    "        \n",
    "        # Analyze silence intervals\n",
    "        silence_intervals = self._get_intervals(silent_frames)\n",
    "        \n",
    "        if len(silence_intervals) > 0:\n",
    "            avg_silence_duration = np.mean([end - start for start, end in silence_intervals])\n",
    "            silence_uniformity = 1.0 - np.std([end - start for start, end in silence_intervals]) / (avg_silence_duration + 1e-8)\n",
    "        else:\n",
    "            avg_silence_duration = 0\n",
    "            silence_uniformity = 0\n",
    "        \n",
    "        return {\n",
    "            'silence_ratio': float(silence_ratio),\n",
    "            'num_silence_intervals': len(silence_intervals),\n",
    "            'avg_silence_duration': float(avg_silence_duration),\n",
    "            'silence_uniformity': float(silence_uniformity)\n",
    "        }\n",
    "    \n",
    "    def _analyze_breaths(self, y, sr):\n",
    "        \"\"\"Analyze breath-like sounds.\"\"\"\n",
    "        # Breaths typically have specific spectral characteristics\n",
    "        # High frequency content, low amplitude, short duration\n",
    "        \n",
    "        # Calculate spectral centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        \n",
    "        # Calculate zero crossing rate (breaths have higher ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        \n",
    "        # Calculate RMS\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        \n",
    "        # Detect potential breath events\n",
    "        # Breaths: high ZCR + low RMS + moderate spectral centroid\n",
    "        rms_threshold = np.percentile(rms, 30)\n",
    "        zcr_threshold = np.percentile(zcr, 70)\n",
    "        \n",
    "        potential_breaths = (zcr > zcr_threshold) & (rms < rms_threshold)\n",
    "        \n",
    "        breath_ratio = np.mean(potential_breaths)\n",
    "        num_breath_events = len(self._get_intervals(potential_breaths))\n",
    "        \n",
    "        return {\n",
    "            'breath_ratio': float(breath_ratio),\n",
    "            'num_breath_events': num_breath_events,\n",
    "            'avg_zcr': float(np.mean(zcr))\n",
    "        }\n",
    "    \n",
    "    def _analyze_spectral(self, y, sr):\n",
    "        \"\"\"Analyze spectral characteristics.\"\"\"\n",
    "        # Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "        \n",
    "        return {\n",
    "            'spectral_centroid_mean': float(np.mean(spectral_centroid)),\n",
    "            'spectral_centroid_std': float(np.std(spectral_centroid)),\n",
    "            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),\n",
    "            'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),\n",
    "            'spectral_bandwidth_std': float(np.std(spectral_bandwidth))\n",
    "        }\n",
    "    \n",
    "    def _analyze_energy(self, y, sr):\n",
    "        \"\"\"Analyze energy patterns.\"\"\"\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        \n",
    "        # Energy statistics\n",
    "        energy_mean = np.mean(rms)\n",
    "        energy_std = np.std(rms)\n",
    "        energy_variance = np.var(rms)\n",
    "        \n",
    "        # Energy consistency (AI voices tend to have more consistent energy)\n",
    "        energy_consistency = 1.0 / (1.0 + energy_variance * 100)\n",
    "        \n",
    "        return {\n",
    "            'energy_mean': float(energy_mean),\n",
    "            'energy_std': float(energy_std),\n",
    "            'energy_variance': float(energy_variance),\n",
    "            'energy_consistency': float(energy_consistency)\n",
    "        }\n",
    "    \n",
    "    def _get_intervals(self, boolean_array):\n",
    "        \"\"\"Get start and end indices of True intervals.\"\"\"\n",
    "        intervals = []\n",
    "        in_interval = False\n",
    "        start = 0\n",
    "        \n",
    "        for i, val in enumerate(boolean_array):\n",
    "            if val and not in_interval:\n",
    "                start = i\n",
    "                in_interval = True\n",
    "            elif not val and in_interval:\n",
    "                intervals.append((start, i))\n",
    "                in_interval = False\n",
    "        \n",
    "        if in_interval:\n",
    "            intervals.append((start, len(boolean_array)))\n",
    "        \n",
    "        return intervals\n",
    "    \n",
    "    def _get_default_analysis(self):\n",
    "        \"\"\"Return default analysis in case of error.\"\"\"\n",
    "        return {\n",
    "            'pitch_variance': 0,\n",
    "            'pitch_consistency': 0,\n",
    "            'pitch_range': 0,\n",
    "            'silence_ratio': 0,\n",
    "            'num_silence_intervals': 0,\n",
    "            'avg_silence_duration': 0,\n",
    "            'silence_uniformity': 0,\n",
    "            'breath_ratio': 0,\n",
    "            'num_breath_events': 0,\n",
    "            'avg_zcr': 0,\n",
    "            'spectral_centroid_mean': 0,\n",
    "            'energy_consistency': 0\n",
    "        }\n",
    "\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    \"\"\"\n",
    "    Generates human-readable explanations based on audio analysis and prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = AudioAnalyzer()\n",
    "    \n",
    "    def generate_explanation(self, audio_path, prediction_class, confidence_score):\n",
    "        \"\"\"\n",
    "        Generate explanation based on audio analysis.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            prediction_class: 'REAL' or 'AI_GENERATED'\n",
    "            confidence_score: Confidence score (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            Explanation string\n",
    "        \"\"\"\n",
    "        # Analyze audio\n",
    "        analysis = self.analyzer.analyze_audio(audio_path)\n",
    "        \n",
    "        # Generate explanation based on class and features\n",
    "        if prediction_class == 'AI_GENERATED':\n",
    "            explanation = self._explain_ai_generated(analysis, confidence_score)\n",
    "        else:\n",
    "            explanation = self._explain_real(analysis, confidence_score)\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _explain_ai_generated(self, analysis, confidence):\n",
    "        \"\"\"Generate explanation for AI-generated classification.\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        # Check pitch consistency (AI tends to be more consistent)\n",
    "        if analysis['pitch_consistency'] > 0.7:\n",
    "            reasons.append(\"unnatural pitch consistency\")\n",
    "        \n",
    "        # Check pitch variance (AI tends to have lower variance)\n",
    "        if analysis['pitch_variance'] < 500:\n",
    "            reasons.append(\"limited pitch variation\")\n",
    "        \n",
    "        # Check silence patterns (AI tends to have uniform silences)\n",
    "        if analysis['silence_uniformity'] > 0.7:\n",
    "            reasons.append(\"uniform silence patterns\")\n",
    "        \n",
    "        # Check breath events (AI tends to lack natural breaths)\n",
    "        if analysis['num_breath_events'] < 2:\n",
    "            reasons.append(\"absence of natural breath sounds\")\n",
    "        \n",
    "        # Check energy consistency (AI tends to be more consistent)\n",
    "        if analysis['energy_consistency'] > 0.7:\n",
    "            reasons.append(\"robotic energy patterns\")\n",
    "        \n",
    "        # Check spectral characteristics\n",
    "        if analysis['spectral_bandwidth_std'] < 200:\n",
    "            reasons.append(\"limited spectral variation\")\n",
    "        \n",
    "        # Construct explanation\n",
    "        if len(reasons) == 0:\n",
    "            explanation = \"Artificial speech patterns detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            explanation = f\"{reasons[0].capitalize()} detected\"\n",
    "        elif len(reasons) == 2:\n",
    "            explanation = f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "        else:\n",
    "            explanation = f\"{reasons[0].capitalize()}, {reasons[1]}, and {reasons[2]} detected\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _explain_real(self, analysis, confidence):\n",
    "        \"\"\"Generate explanation for real voice classification.\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        # Check pitch variation (human tends to have more variation)\n",
    "        if analysis['pitch_variance'] > 1000:\n",
    "            reasons.append(\"natural pitch variation\")\n",
    "        \n",
    "        # Check breath events (humans have breath sounds)\n",
    "        if analysis['num_breath_events'] >= 2:\n",
    "            reasons.append(\"natural breath patterns\")\n",
    "        \n",
    "        # Check energy patterns (humans have less consistent energy)\n",
    "        if analysis['energy_consistency'] < 0.5:\n",
    "            reasons.append(\"organic energy fluctuations\")\n",
    "        \n",
    "        # Check silence patterns (humans have varied pauses)\n",
    "        if analysis['silence_uniformity'] < 0.5:\n",
    "            reasons.append(\"natural pause patterns\")\n",
    "        \n",
    "        # Check spectral variation\n",
    "        if analysis['spectral_bandwidth_std'] > 300:\n",
    "            reasons.append(\"rich spectral dynamics\")\n",
    "        \n",
    "        # Construct explanation\n",
    "        if len(reasons) == 0:\n",
    "            explanation = \"Human speech characteristics detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            explanation = f\"{reasons[0].capitalize()} detected\"\n",
    "        elif len(reasons) == 2:\n",
    "            explanation = f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "        else:\n",
    "            explanation = f\"{reasons[0].capitalize()}, {reasons[1]}, and {reasons[2]} detected\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "\n",
    "# Test the explanation engine\n",
    "print(\"\\n[Testing Explanation Engine]\")\n",
    "\n",
    "analyzer = AudioAnalyzer()\n",
    "explainer = ExplanationGenerator()\n",
    "\n",
    "print(\"✓ AudioAnalyzer initialized\")\n",
    "print(\"✓ ExplanationGenerator initialized\")\n",
    "print(\"\\nReady to generate explanations for predictions!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7cf72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T03:37:37.255657Z",
     "iopub.status.busy": "2026-02-10T03:37:37.255112Z",
     "iopub.status.idle": "2026-02-10T03:37:37.434998Z",
     "shell.execute_reply": "2026-02-10T03:37:37.434186Z"
    },
    "papermill": {
     "duration": 1.809692,
     "end_time": "2026-02-10T03:37:37.436834",
     "exception": false,
     "start_time": "2026-02-10T03:37:35.627142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 5: INFERENCE PIPELINE\n",
      "============================================================\n",
      "\n",
      "[Step 1/2] Initializing inference pipeline...\n",
      "\n",
      "[Initializing Pipeline]\n",
      "Loading model...\n",
      "✓ Model loaded\n",
      "Loading normalization parameters...\n",
      "✓ Normalization parameters loaded\n",
      "✓ Language detector initialized\n",
      "✓ Explanation generator initialized\n",
      "\n",
      "✓ Pipeline ready!\n",
      "\n",
      "[Step 2/2] Pipeline ready for inference!\n",
      "\n",
      "Usage example:\n",
      "============================================================\n",
      "# Single prediction\n",
      "result = pipeline.predict('path/to/audio.wav')\n",
      "print(json.dumps(result, indent=2))\n",
      "\n",
      "# Batch prediction\n",
      "results = pipeline.predict_batch(['audio1.wav', 'audio2.wav'])\n",
      "for result in results:\n",
      "    print(json.dumps(result, indent=2))\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 5: Language Detection and Inference Pipeline (FIXED - All-in-One)\n",
    "Complete pipeline for making predictions with language detection and explanations.\n",
    "All dependencies included - no imports from other blocks needed.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import json\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 5: INFERENCE PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# AudioAnalyzer Class (from Block 4)\n",
    "# ============================================================================\n",
    "\n",
    "class AudioAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes audio files to extract interpretable features for explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def analyze_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract various audio features for rule-based explanation.\n",
    "        Returns a dictionary of analysis results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=10)\n",
    "            \n",
    "            analysis = {}\n",
    "            \n",
    "            # 1. PITCH ANALYSIS\n",
    "            pitch_features = self._analyze_pitch(y, sr)\n",
    "            analysis.update(pitch_features)\n",
    "            \n",
    "            # 2. SILENCE ANALYSIS\n",
    "            silence_features = self._analyze_silence(y, sr)\n",
    "            analysis.update(silence_features)\n",
    "            \n",
    "            # 3. BREATH ANALYSIS\n",
    "            breath_features = self._analyze_breaths(y, sr)\n",
    "            analysis.update(breath_features)\n",
    "            \n",
    "            # 4. SPECTRAL ANALYSIS\n",
    "            spectral_features = self._analyze_spectral(y, sr)\n",
    "            analysis.update(spectral_features)\n",
    "            \n",
    "            # 5. ENERGY ANALYSIS\n",
    "            energy_features = self._analyze_energy(y, sr)\n",
    "            analysis.update(energy_features)\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing audio: {e}\")\n",
    "            return self._get_default_analysis()\n",
    "    \n",
    "    def _analyze_pitch(self, y, sr):\n",
    "        \"\"\"Analyze pitch characteristics.\"\"\"\n",
    "        # Extract pitch using piptrack\n",
    "        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "        \n",
    "        # Get pitch values with significant magnitude\n",
    "        pitch_values = []\n",
    "        for t in range(pitches.shape[1]):\n",
    "            index = magnitudes[:, t].argmax()\n",
    "            pitch = pitches[index, t]\n",
    "            if pitch > 0:\n",
    "                pitch_values.append(pitch)\n",
    "        \n",
    "        if len(pitch_values) == 0:\n",
    "            return {\n",
    "                'pitch_variance': 0,\n",
    "                'pitch_consistency': 0,\n",
    "                'pitch_range': 0\n",
    "            }\n",
    "        \n",
    "        pitch_values = np.array(pitch_values)\n",
    "        \n",
    "        # Calculate pitch statistics\n",
    "        pitch_variance = np.var(pitch_values)\n",
    "        pitch_std = np.std(pitch_values)\n",
    "        pitch_mean = np.mean(pitch_values)\n",
    "        pitch_range = np.max(pitch_values) - np.min(pitch_values)\n",
    "        \n",
    "        # Consistency score (lower variance = more consistent/robotic)\n",
    "        pitch_consistency = 1.0 / (1.0 + pitch_variance / 1000)\n",
    "        \n",
    "        return {\n",
    "            'pitch_variance': float(pitch_variance),\n",
    "            'pitch_consistency': float(pitch_consistency),\n",
    "            'pitch_range': float(pitch_range),\n",
    "            'pitch_mean': float(pitch_mean),\n",
    "            'pitch_std': float(pitch_std)\n",
    "        }\n",
    "    \n",
    "    def _analyze_silence(self, y, sr):\n",
    "        \"\"\"Analyze silence patterns.\"\"\"\n",
    "        # Split audio into frames\n",
    "        frame_length = 2048\n",
    "        hop_length = 512\n",
    "        \n",
    "        # Calculate RMS energy\n",
    "        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "        \n",
    "        # Define silence threshold (adaptive)\n",
    "        silence_threshold = np.percentile(rms, 20)\n",
    "        \n",
    "        # Find silent frames\n",
    "        silent_frames = rms < silence_threshold\n",
    "        silence_ratio = np.mean(silent_frames)\n",
    "        \n",
    "        # Analyze silence intervals\n",
    "        silence_intervals = self._get_intervals(silent_frames)\n",
    "        \n",
    "        if len(silence_intervals) > 0:\n",
    "            avg_silence_duration = np.mean([end - start for start, end in silence_intervals])\n",
    "            silence_uniformity = 1.0 - np.std([end - start for start, end in silence_intervals]) / (avg_silence_duration + 1e-8)\n",
    "        else:\n",
    "            avg_silence_duration = 0\n",
    "            silence_uniformity = 0\n",
    "        \n",
    "        return {\n",
    "            'silence_ratio': float(silence_ratio),\n",
    "            'num_silence_intervals': len(silence_intervals),\n",
    "            'avg_silence_duration': float(avg_silence_duration),\n",
    "            'silence_uniformity': float(silence_uniformity)\n",
    "        }\n",
    "    \n",
    "    def _analyze_breaths(self, y, sr):\n",
    "        \"\"\"Analyze breath-like sounds.\"\"\"\n",
    "        # Breaths typically have specific spectral characteristics\n",
    "        # High frequency content, low amplitude, short duration\n",
    "        \n",
    "        # Calculate spectral centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        \n",
    "        # Calculate zero crossing rate (breaths have higher ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "        \n",
    "        # Calculate RMS\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        \n",
    "        # Detect potential breath events\n",
    "        # Breaths: high ZCR + low RMS + moderate spectral centroid\n",
    "        rms_threshold = np.percentile(rms, 30)\n",
    "        zcr_threshold = np.percentile(zcr, 70)\n",
    "        \n",
    "        potential_breaths = (zcr > zcr_threshold) & (rms < rms_threshold)\n",
    "        \n",
    "        breath_ratio = np.mean(potential_breaths)\n",
    "        num_breath_events = len(self._get_intervals(potential_breaths))\n",
    "        \n",
    "        return {\n",
    "            'breath_ratio': float(breath_ratio),\n",
    "            'num_breath_events': num_breath_events,\n",
    "            'avg_zcr': float(np.mean(zcr))\n",
    "        }\n",
    "    \n",
    "    def _analyze_spectral(self, y, sr):\n",
    "        \"\"\"Analyze spectral characteristics.\"\"\"\n",
    "        # Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "        \n",
    "        return {\n",
    "            'spectral_centroid_mean': float(np.mean(spectral_centroid)),\n",
    "            'spectral_centroid_std': float(np.std(spectral_centroid)),\n",
    "            'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),\n",
    "            'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),\n",
    "            'spectral_bandwidth_std': float(np.std(spectral_bandwidth))\n",
    "        }\n",
    "    \n",
    "    def _analyze_energy(self, y, sr):\n",
    "        \"\"\"Analyze energy patterns.\"\"\"\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        \n",
    "        # Energy statistics\n",
    "        energy_mean = np.mean(rms)\n",
    "        energy_std = np.std(rms)\n",
    "        energy_variance = np.var(rms)\n",
    "        \n",
    "        # Energy consistency (AI voices tend to have more consistent energy)\n",
    "        energy_consistency = 1.0 / (1.0 + energy_variance * 100)\n",
    "        \n",
    "        return {\n",
    "            'energy_mean': float(energy_mean),\n",
    "            'energy_std': float(energy_std),\n",
    "            'energy_variance': float(energy_variance),\n",
    "            'energy_consistency': float(energy_consistency)\n",
    "        }\n",
    "    \n",
    "    def _get_intervals(self, boolean_array):\n",
    "        \"\"\"Get start and end indices of True intervals.\"\"\"\n",
    "        intervals = []\n",
    "        in_interval = False\n",
    "        start = 0\n",
    "        \n",
    "        for i, val in enumerate(boolean_array):\n",
    "            if val and not in_interval:\n",
    "                start = i\n",
    "                in_interval = True\n",
    "            elif not val and in_interval:\n",
    "                intervals.append((start, i))\n",
    "                in_interval = False\n",
    "        \n",
    "        if in_interval:\n",
    "            intervals.append((start, len(boolean_array)))\n",
    "        \n",
    "        return intervals\n",
    "    \n",
    "    def _get_default_analysis(self):\n",
    "        \"\"\"Return default analysis in case of error.\"\"\"\n",
    "        return {\n",
    "            'pitch_variance': 0,\n",
    "            'pitch_consistency': 0,\n",
    "            'pitch_range': 0,\n",
    "            'silence_ratio': 0,\n",
    "            'num_silence_intervals': 0,\n",
    "            'avg_silence_duration': 0,\n",
    "            'silence_uniformity': 0,\n",
    "            'breath_ratio': 0,\n",
    "            'num_breath_events': 0,\n",
    "            'avg_zcr': 0,\n",
    "            'spectral_centroid_mean': 0,\n",
    "            'energy_consistency': 0\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ExplanationGenerator Class (from Block 4)\n",
    "# ============================================================================\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    \"\"\"\n",
    "    Generates human-readable explanations based on audio analysis and prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = AudioAnalyzer()\n",
    "    \n",
    "    def generate_explanation(self, audio_path, prediction_class, confidence_score):\n",
    "        \"\"\"\n",
    "        Generate explanation based on audio analysis.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            prediction_class: 'REAL' or 'AI_GENERATED'\n",
    "            confidence_score: Confidence score (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            Explanation string\n",
    "        \"\"\"\n",
    "        # Analyze audio\n",
    "        analysis = self.analyzer.analyze_audio(audio_path)\n",
    "        \n",
    "        # Generate explanation based on class and features\n",
    "        if prediction_class == 'AI_GENERATED':\n",
    "            explanation = self._explain_ai_generated(analysis, confidence_score)\n",
    "        else:\n",
    "            explanation = self._explain_real(analysis, confidence_score)\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _explain_ai_generated(self, analysis, confidence):\n",
    "        \"\"\"Generate explanation for AI-generated classification.\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        # Check pitch consistency (AI tends to be more consistent)\n",
    "        if analysis['pitch_consistency'] > 0.7:\n",
    "            reasons.append(\"unnatural pitch consistency\")\n",
    "        \n",
    "        # Check pitch variance (AI tends to have lower variance)\n",
    "        if analysis['pitch_variance'] < 500:\n",
    "            reasons.append(\"limited pitch variation\")\n",
    "        \n",
    "        # Check silence patterns (AI tends to have uniform silences)\n",
    "        if analysis['silence_uniformity'] > 0.7:\n",
    "            reasons.append(\"uniform silence patterns\")\n",
    "        \n",
    "        # Check breath events (AI tends to lack natural breaths)\n",
    "        if analysis['num_breath_events'] < 2:\n",
    "            reasons.append(\"absence of natural breath sounds\")\n",
    "        \n",
    "        # Check energy consistency (AI tends to be more consistent)\n",
    "        if analysis['energy_consistency'] > 0.7:\n",
    "            reasons.append(\"robotic energy patterns\")\n",
    "        \n",
    "        # Check spectral characteristics\n",
    "        if analysis['spectral_bandwidth_std'] < 200:\n",
    "            reasons.append(\"limited spectral variation\")\n",
    "        \n",
    "        # Construct explanation\n",
    "        if len(reasons) == 0:\n",
    "            explanation = \"Artificial speech patterns detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            explanation = f\"{reasons[0].capitalize()} detected\"\n",
    "        elif len(reasons) == 2:\n",
    "            explanation = f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "        else:\n",
    "            explanation = f\"{reasons[0].capitalize()}, {reasons[1]}, and {reasons[2]} detected\"\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def _explain_real(self, analysis, confidence):\n",
    "        \"\"\"Generate explanation for real voice classification.\"\"\"\n",
    "        reasons = []\n",
    "        \n",
    "        # Check pitch variation (human tends to have more variation)\n",
    "        if analysis['pitch_variance'] > 1000:\n",
    "            reasons.append(\"natural pitch variation\")\n",
    "        \n",
    "        # Check breath events (humans have breath sounds)\n",
    "        if analysis['num_breath_events'] >= 2:\n",
    "            reasons.append(\"natural breath patterns\")\n",
    "        \n",
    "        # Check energy patterns (humans have less consistent energy)\n",
    "        if analysis['energy_consistency'] < 0.5:\n",
    "            reasons.append(\"organic energy fluctuations\")\n",
    "        \n",
    "        # Check silence patterns (humans have varied pauses)\n",
    "        if analysis['silence_uniformity'] < 0.5:\n",
    "            reasons.append(\"natural pause patterns\")\n",
    "        \n",
    "        # Check spectral variation\n",
    "        if analysis['spectral_bandwidth_std'] > 300:\n",
    "            reasons.append(\"rich spectral dynamics\")\n",
    "        \n",
    "        # Construct explanation\n",
    "        if len(reasons) == 0:\n",
    "            explanation = \"Human speech characteristics detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            explanation = f\"{reasons[0].capitalize()} detected\"\n",
    "        elif len(reasons) == 2:\n",
    "            explanation = f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "        else:\n",
    "            explanation = f\"{reasons[0].capitalize()}, {reasons[1]}, and {reasons[2]} detected\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LanguageDetector Class\n",
    "# ============================================================================\n",
    "\n",
    "class LanguageDetector:\n",
    "    \"\"\"\n",
    "    Detects language from audio file.\n",
    "    Uses a simple heuristic based on dataset structure.\n",
    "    For production, you would use a proper language detection model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Map language codes to full names\n",
    "        self.language_map = {\n",
    "            'ta': 'Tamil',\n",
    "            'hi': 'Hindi',\n",
    "            'te': 'Telugu',\n",
    "            'ml': 'Malayalam',\n",
    "            'kn': 'Kannada',\n",
    "            'en': 'English'\n",
    "        }\n",
    "    \n",
    "    def detect_from_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Detect language from filename pattern.\n",
    "        Assumes filenames contain language codes.\n",
    "        \"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        \n",
    "        # Check for language codes in filename\n",
    "        for code, language in self.language_map.items():\n",
    "            if code in filename_lower:\n",
    "                return language\n",
    "        \n",
    "        # Default to English if not detected\n",
    "        return \"English\"\n",
    "    \n",
    "    def detect_from_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        Detect language from audio characteristics.\n",
    "        This is a placeholder - in production, use a proper language detection model.\n",
    "        \"\"\"\n",
    "        # For this implementation, we'll try to detect from filename\n",
    "        import os\n",
    "        filename = os.path.basename(audio_path)\n",
    "        return self.detect_from_filename(filename)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VoiceClassificationPipeline Class\n",
    "# ============================================================================\n",
    "\n",
    "class VoiceClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for voice classification with explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, normalization_path):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to trained model (.h5 file)\n",
    "            normalization_path: Path to normalization parameters (.npz file)\n",
    "        \"\"\"\n",
    "        print(\"\\n[Initializing Pipeline]\")\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "        print(\"✓ Model loaded\")\n",
    "        \n",
    "        # Load normalization parameters\n",
    "        print(\"Loading normalization parameters...\")\n",
    "        norm_params = np.load(normalization_path)\n",
    "        self.norm_mean = norm_params['mean']\n",
    "        self.norm_std = norm_params['std']\n",
    "        print(\"✓ Normalization parameters loaded\")\n",
    "        \n",
    "        # Initialize language detector\n",
    "        self.language_detector = LanguageDetector()\n",
    "        print(\"✓ Language detector initialized\")\n",
    "        \n",
    "        # Initialize explanation generator\n",
    "        self.explainer = ExplanationGenerator()\n",
    "        print(\"✓ Explanation generator initialized\")\n",
    "        \n",
    "        # Feature extraction parameters (must match training)\n",
    "        self.sample_rate = 16000\n",
    "        self.n_lfcc = 40\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.max_duration = 10\n",
    "        \n",
    "        print(\"\\n✓ Pipeline ready!\")\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        \"\"\"Extract LFCC features from audio file.\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.max_duration)\n",
    "            \n",
    "            # Compute LFCC\n",
    "            lfcc = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_lfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Standardize to fixed length\n",
    "            target_length = int(self.max_duration * sr / self.hop_length)\n",
    "            \n",
    "            if lfcc.shape[1] < target_length:\n",
    "                pad_width = target_length - lfcc.shape[1]\n",
    "                lfcc = np.pad(lfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "            else:\n",
    "                lfcc = lfcc[:, :target_length]\n",
    "            \n",
    "            return lfcc\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def predict(self, audio_path):\n",
    "        \"\"\"\n",
    "        Make prediction on a single audio file.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with prediction results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract features\n",
    "            features = self.extract_features(audio_path)\n",
    "            if features is None:\n",
    "                return self._get_error_response(\"Feature extraction failed\")\n",
    "            \n",
    "            # Reshape and normalize\n",
    "            features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "            features = np.expand_dims(features, axis=-1)  # Add channel dimension\n",
    "            features = (features - self.norm_mean) / (self.norm_std + 1e-8)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction_proba = self.model.predict(features, verbose=0)[0][0]\n",
    "            \n",
    "            # Convert to class\n",
    "            if prediction_proba > 0.5:\n",
    "                classification = \"AI_GENERATED\"\n",
    "                confidence_score = float(prediction_proba)\n",
    "            else:\n",
    "                classification = \"REAL\"\n",
    "                confidence_score = float(1.0 - prediction_proba)\n",
    "            \n",
    "            # Detect language\n",
    "            language = self.language_detector.detect_from_audio(audio_path)\n",
    "            \n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.generate_explanation(\n",
    "                audio_path, \n",
    "                classification, \n",
    "                confidence_score\n",
    "            )\n",
    "            \n",
    "            # Construct response\n",
    "            response = {\n",
    "                \"status\": \"success\",\n",
    "                \"language\": language,\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": round(confidence_score, 2),\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "            \n",
    "            return response\n",
    "        \n",
    "        except Exception as e:\n",
    "            return self._get_error_response(str(e))\n",
    "    \n",
    "    def predict_batch(self, audio_paths):\n",
    "        \"\"\"\n",
    "        Make predictions on multiple audio files.\n",
    "        \n",
    "        Args:\n",
    "            audio_paths: List of audio file paths\n",
    "        \n",
    "        Returns:\n",
    "            List of prediction dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for audio_path in audio_paths:\n",
    "            result = self.predict(audio_path)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def _get_error_response(self, error_message):\n",
    "        \"\"\"Return error response.\"\"\"\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"language\": \"Unknown\",\n",
    "            \"classification\": \"Unknown\",\n",
    "            \"confidenceScore\": 0.0,\n",
    "            \"explanation\": f\"Error: {error_message}\"\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize the pipeline\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[Step 1/2] Initializing inference pipeline...\")\n",
    "\n",
    "pipeline = VoiceClassificationPipeline(\n",
    "    model_path='checkpoints/best_model.h5',\n",
    "    normalization_path='normalization_params.npz'\n",
    ")\n",
    "\n",
    "print(\"\\n[Step 2/2] Pipeline ready for inference!\")\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"# Single prediction\")\n",
    "print(\"result = pipeline.predict('path/to/audio.wav')\")\n",
    "print(\"print(json.dumps(result, indent=2))\")\n",
    "print()\n",
    "print(\"# Batch prediction\")\n",
    "print(\"results = pipeline.predict_batch(['audio1.wav', 'audio2.wav'])\")\n",
    "print(\"for result in results:\")\n",
    "print(\"    print(json.dumps(result, indent=2))\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e489f286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T03:37:40.702852Z",
     "iopub.status.busy": "2026-02-10T03:37:40.702566Z",
     "iopub.status.idle": "2026-02-10T04:59:03.478190Z",
     "shell.execute_reply": "2026-02-10T04:59:03.477110Z"
    },
    "papermill": {
     "duration": 4884.394709,
     "end_time": "2026-02-10T04:59:03.479824",
     "exception": false,
     "start_time": "2026-02-10T03:37:39.085115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 6: TESTING AND EVALUATION\n",
      "============================================================\n",
      "\n",
      "✓ Pipeline found from previous block\n",
      "\n",
      "[Step 1/5] Loading test dataset...\n",
      "Test samples: 27200\n",
      "\n",
      "[Step 2/5] Testing on sample files...\n",
      "\n",
      "Example Predictions:\n",
      "============================================================\n",
      "\n",
      "Sample 8923:\n",
      "File: ma_common_voice_ml_37863448.mp3\n",
      "True Label: AI_GENERATED\n",
      "Prediction:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"language\": \"Malayalam\",\n",
      "  \"classification\": \"AI_GENERATED\",\n",
      "  \"confidenceScore\": 1.0,\n",
      "  \"explanation\": \"Uniform silence patterns detected\"\n",
      "}\n",
      "Correct: ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 25536:\n",
      "File: ma_mlm_08822_01297943779.mp3\n",
      "True Label: REAL\n",
      "Prediction:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"language\": \"Malayalam\",\n",
      "  \"classification\": \"REAL\",\n",
      "  \"confidenceScore\": 1.0,\n",
      "  \"explanation\": \"Natural pitch variation, natural breath patterns, and rich spectral dynamics detected\"\n",
      "}\n",
      "Correct: ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 21347:\n",
      "File: ma_mlf_03435_00517990460.mp3\n",
      "True Label: AI_GENERATED\n",
      "Prediction:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"language\": \"Malayalam\",\n",
      "  \"classification\": \"AI_GENERATED\",\n",
      "  \"confidenceScore\": 1.0,\n",
      "  \"explanation\": \"Artificial speech patterns detected\"\n",
      "}\n",
      "Correct: ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 10300:\n",
      "File: te_IISc_SYSPINProject_te_m_OTHE_01210.mp3\n",
      "True Label: AI_GENERATED\n",
      "Prediction:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"language\": \"Telugu\",\n",
      "  \"classification\": \"AI_GENERATED\",\n",
      "  \"confidenceScore\": 1.0,\n",
      "  \"explanation\": \"Artificial speech patterns detected\"\n",
      "}\n",
      "Correct: ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample 13493:\n",
      "File: ta_common_voice_ta_19305288.mp3\n",
      "True Label: AI_GENERATED\n",
      "Prediction:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"language\": \"Tamil\",\n",
      "  \"classification\": \"AI_GENERATED\",\n",
      "  \"confidenceScore\": 1.0,\n",
      "  \"explanation\": \"Artificial speech patterns detected\"\n",
      "}\n",
      "Correct: ✓\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Step 3/5] Running full evaluation on test set...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 27200/27200 [1:21:20<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 4/5] Evaluation Results:\n",
      "============================================================\n",
      "Accuracy:  0.9999 (99.99%)\n",
      "Precision: 0.9998\n",
      "Recall:    1.0000\n",
      "F1-Score:  0.9999\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted REAL  Predicted AI\n",
      "Actual REAL               13597             3\n",
      "Actual AI                     0         13600\n",
      "\n",
      "Average Confidence: 0.9999\n",
      "Confidence Std Dev: 0.0056\n",
      "\n",
      "Language Distribution:\n",
      "  English: 7763 samples (28.5%)\n",
      "  Telugu: 6769 samples (24.9%)\n",
      "  Hindi: 5175 samples (19.0%)\n",
      "  Tamil: 4227 samples (15.5%)\n",
      "  Malayalam: 3266 samples (12.0%)\n",
      "\n",
      "[Step 5/5] Saving Results...\n",
      "✓ Saved detailed predictions to: test_predictions.csv\n",
      "✓ Saved evaluation summary to: evaluation_summary.json\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Key Files Generated:\n",
      "  • test_predictions.csv - Detailed predictions for all test samples\n",
      "  • evaluation_summary.json - Summary metrics and statistics\n",
      "\n",
      "You can now use the pipeline for inference on new audio files!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 6: Testing and Evaluation (FIXED - No External Imports)\n",
    "Test the complete pipeline and generate example predictions.\n",
    "Run this AFTER running block5_inference_pipeline_fixed.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 6: TESTING AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if pipeline exists (from previous block)\n",
    "try:\n",
    "    # This should exist if block5 was run\n",
    "    pipeline\n",
    "    print(\"\\n✓ Pipeline found from previous block\")\n",
    "except NameError:\n",
    "    print(\"\\n✗ ERROR: Pipeline not found!\")\n",
    "    print(\"Please run block5_inference_pipeline_fixed.py first\")\n",
    "    print(\"\\nTo fix this, run:\")\n",
    "    print(\"  %run block5_inference_pipeline_fixed.py\")\n",
    "    raise SystemExit(\"Pipeline not initialized\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\n[Step 1/5] Loading test dataset...\")\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Test on a few samples\n",
    "print(\"\\n[Step 2/5] Testing on sample files...\")\n",
    "print(\"\\nExample Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select a few random samples for demonstration\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_df), size=min(5, len(test_df)), replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = test_df.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nSample {idx + 1}:\")\n",
    "    print(f\"File: {row['filename']}\")\n",
    "    print(f\"True Label: {row['label_text']}\")\n",
    "    \n",
    "    # Make prediction\n",
    "    result = pipeline.predict(row['filepath'])\n",
    "    \n",
    "    # Display result in JSON format\n",
    "    print(f\"Prediction:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "    \n",
    "    # Check if correct\n",
    "    is_correct = result['classification'] == row['label_text']\n",
    "    print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Full evaluation on test set\n",
    "print(\"\\n[Step 3/5] Running full evaluation on test set...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing\"):\n",
    "    result = pipeline.predict(row['filepath'])\n",
    "    \n",
    "    all_predictions.append(result['classification'])\n",
    "    all_true_labels.append(row['label_text'])\n",
    "    all_results.append(result)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Convert to binary (0=REAL, 1=AI_GENERATED)\n",
    "y_true = [1 if label == 'AI_GENERATED' else 0 for label in all_true_labels]\n",
    "y_pred = [1 if label == 'AI_GENERATED' else 0 for label in all_predictions]\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n[Step 4/5] Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted REAL  Predicted AI\")\n",
    "print(f\"Actual REAL      {cm[0][0]:14d}  {cm[0][1]:12d}\")\n",
    "print(f\"Actual AI        {cm[1][0]:14d}  {cm[1][1]:12d}\")\n",
    "\n",
    "# Analyze confidence scores\n",
    "all_confidences = [result['confidenceScore'] for result in all_results]\n",
    "avg_confidence = np.mean(all_confidences)\n",
    "std_confidence = np.std(all_confidences)\n",
    "\n",
    "print(f\"\\nAverage Confidence: {avg_confidence:.4f}\")\n",
    "print(f\"Confidence Std Dev: {std_confidence:.4f}\")\n",
    "\n",
    "# Language distribution\n",
    "language_counts = {}\n",
    "for result in all_results:\n",
    "    lang = result['language']\n",
    "    language_counts[lang] = language_counts.get(lang, 0) + 1\n",
    "\n",
    "print(\"\\nLanguage Distribution:\")\n",
    "for lang, count in sorted(language_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {lang}: {count} samples ({count/len(all_results)*100:.1f}%)\")\n",
    "\n",
    "# Save detailed results\n",
    "print(\"\\n[Step 5/5] Saving Results...\")\n",
    "\n",
    "# Save predictions to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'filename': test_df['filename'].values,\n",
    "    'true_label': all_true_labels,\n",
    "    'predicted_label': all_predictions,\n",
    "    'confidence': all_confidences,\n",
    "    'language': [r['language'] for r in all_results],\n",
    "    'explanation': [r['explanation'] for r in all_results]\n",
    "})\n",
    "results_df.to_csv('test_predictions.csv', index=False)\n",
    "print(\"✓ Saved detailed predictions to: test_predictions.csv\")\n",
    "\n",
    "# Save summary metrics\n",
    "summary = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'avg_confidence': float(avg_confidence),\n",
    "    'std_confidence': float(std_confidence),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'language_distribution': language_counts,\n",
    "    'total_samples': len(test_df)\n",
    "}\n",
    "\n",
    "with open('evaluation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"✓ Saved evaluation summary to: evaluation_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey Files Generated:\")\n",
    "print(\"  • test_predictions.csv - Detailed predictions for all test samples\")\n",
    "print(\"  • evaluation_summary.json - Summary metrics and statistics\")\n",
    "print(\"\\nYou can now use the pipeline for inference on new audio files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b676a740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T04:59:08.883455Z",
     "iopub.status.busy": "2026-02-10T04:59:08.883056Z",
     "iopub.status.idle": "2026-02-10T04:59:09.166653Z",
     "shell.execute_reply": "2026-02-10T04:59:09.165495Z"
    },
    "papermill": {
     "duration": 3.043395,
     "end_time": "2026-02-10T04:59:09.167903",
     "exception": true,
     "start_time": "2026-02-10T04:59:06.124508",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BLOCK 6.5: QUICK MODEL SAVE\n",
      "============================================================\n",
      "\n",
      "[Option 1: Save with timestamp]\n",
      "\n",
      "[Step 1/3] Loading best model...\n",
      "✓ Model loaded\n",
      "\n",
      "[Step 2/3] Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: model_saved_20260210_045908/model.h5\n",
      "\n",
      "[Step 3/3] Copying normalization parameters...\n",
      "✓ Saved: model_saved_20260210_045908/normalization_params.npz\n",
      "\n",
      "✓ Model saved to: model_saved_20260210_045908/\n",
      "\n",
      "============================================================\n",
      "[Option 2: Save to 'final_model' directory]\n",
      "\n",
      "[Step 1/2] Saving model...\n",
      "✓ Saved: final_model/model.h5\n",
      "\n",
      "[Step 2/2] Copying normalization parameters...\n",
      "✓ Saved: final_model/normalization_params.npz\n",
      "\n",
      "✓ Model saved to: final_model/\n",
      "\n",
      "============================================================\n",
      "SAVE COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your model has been saved to TWO locations:\n",
      "\n",
      "1. Timestamped backup:\n",
      "   📁 model_saved_20260210_045908/\n",
      "      ├── model.h5\n",
      "      └── normalization_params.npz\n",
      "\n",
      "2. Final model:\n",
      "   📁 final_model/\n",
      "      ├── model.h5\n",
      "      └── normalization_params.npz\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'choice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/2308915832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \"\"\")\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     print(f\"\"\"      ├── saved_model/\n\u001b[1;32m    108\u001b[0m       \u001b[0;31m├\u001b[0m\u001b[0;31m─\u001b[0m\u001b[0;31m─\u001b[0m \u001b[0mmodel_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'choice' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 6.5 SIMPLE: Quick Model Save\n",
    "A simpler version if you just want to save the model quickly.\n",
    "Run this AFTER Block 3 (training) or Block 6 (evaluation).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLOCK 6.5: QUICK MODEL SAVE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# Option 1: Save to a timestamped directory\n",
    "# ============================================================================\n",
    "print(\"\\n[Option 1: Save with timestamp]\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f'model_saved_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load and save the best model\n",
    "print(\"\\n[Step 1/3] Loading best model...\")\n",
    "model = keras.models.load_model('checkpoints/best_model.h5')\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "print(\"\\n[Step 2/3] Saving model...\")\n",
    "model.save(f'{save_dir}/model.h5')\n",
    "model.save(f'{save_dir}/model.keras')\n",
    "print(f\"✓ Saved: {save_dir}/model.h5\")\n",
    "\n",
    "print(\"\\n[Step 3/3] Copying normalization parameters...\")\n",
    "shutil.copy('normalization_params.npz', f'{save_dir}/normalization_params.npz')\n",
    "print(f\"✓ Saved: {save_dir}/normalization_params.npz\")\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {save_dir}/\")\n",
    "\n",
    "# ============================================================================\n",
    "# Option 2: Save to a simple 'final_model' directory\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"[Option 2: Save to 'final_model' directory]\")\n",
    "\n",
    "final_dir = 'final_model'\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\n[Step 1/2] Saving model...\")\n",
    "model.save(f'{final_dir}/model.h5')\n",
    "print(f\"✓ Saved: {final_dir}/model.h5\")\n",
    "\n",
    "print(\"\\n[Step 2/2] Copying normalization parameters...\")\n",
    "shutil.copy('normalization_params.npz', f'{final_dir}/normalization_params.npz')\n",
    "print(f\"✓ Saved: {final_dir}/normalization_params.npz\")\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {final_dir}/\")\n",
    "\n",
    "# ============================================================================\n",
    "# Option 3: Save additional formats (SavedModel, Weights only)\n",
    "# ============================================================================\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"[Option 3: Additional formats (optional)]\")\n",
    "\n",
    "# choice = input(\"\\nSave in additional formats? (y/n): \").strip().lower()\n",
    "\n",
    "# if choice == 'y':\n",
    "#     print(\"\\n[Saving TensorFlow SavedModel format...]\")\n",
    "#     model.save(f'{final_dir}/saved_model')\n",
    "#     print(f\"✓ Saved: {final_dir}/saved_model/\")\n",
    "    \n",
    "#     print(\"\\n[Saving weights only...]\")\n",
    "#     model.save_weights(f'{final_dir}/model_weights.h5')\n",
    "#     print(f\"✓ Saved: {final_dir}/model_weights.h5\")\n",
    "    \n",
    "#     print(\"\\n[Saving architecture as JSON...]\")\n",
    "#     model_json = model.to_json()\n",
    "#     with open(f'{final_dir}/model_architecture.json', 'w') as f:\n",
    "#         f.write(model_json)\n",
    "#     print(f\"✓ Saved: {final_dir}/model_architecture.json\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVE COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Your model has been saved to TWO locations:\n",
    "\n",
    "1. Timestamped backup:\n",
    "   📁 {save_dir}/\n",
    "      ├── model.h5\n",
    "      └── normalization_params.npz\n",
    "\n",
    "2. Final model:\n",
    "   📁 {final_dir}/\n",
    "      ├── model.h5\n",
    "      └── normalization_params.npz\n",
    "\"\"\")\n",
    "\n",
    "if choice == 'y':\n",
    "    print(f\"\"\"      ├── saved_model/\n",
    "      ├── model_weights.h5\n",
    "      └── model_architecture.json\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "🎯 To use the saved model:\n",
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "model = keras.models.load_model('final_model/model.h5')\n",
    "\n",
    "# Load normalization params\n",
    "norm_params = np.load('final_model/normalization_params.npz')\n",
    "mean = norm_params['mean']\n",
    "std = norm_params['std']\n",
    "\n",
    "# Make predictions\n",
    "# ... (extract features, normalize, predict)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# Create a simple usage guide\n",
    "# ============================================================================\n",
    "usage_guide = f\"\"\"# Model Usage Guide\n",
    "\n",
    "## Saved on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Files\n",
    "- `model.h5` - Trained CNN model\n",
    "- `normalization_params.npz` - Feature normalization parameters\n",
    "\n",
    "## Quick Usage\n",
    "\n",
    "### Option 1: Use with inference script\n",
    "```python\n",
    "from block7_inference_standalone import VoiceClassifier\n",
    "\n",
    "classifier = VoiceClassifier(\n",
    "    model_path='{final_dir}/model.h5',\n",
    "    norm_path='{final_dir}/normalization_params.npz'\n",
    ")\n",
    "\n",
    "result = classifier.classify('audio.wav')\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Option 2: Manual usage\n",
    "```python\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load model and normalization params\n",
    "model = keras.models.load_model('{final_dir}/model.h5')\n",
    "norm_params = np.load('{final_dir}/normalization_params.npz')\n",
    "mean = norm_params['mean']\n",
    "std = norm_params['std']\n",
    "\n",
    "# Extract LFCC features\n",
    "y, sr = librosa.load('audio.wav', sr=16000, duration=10)\n",
    "lfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512)\n",
    "\n",
    "# Pad to fixed length (313 time steps)\n",
    "target_length = 313\n",
    "if lfcc.shape[1] < target_length:\n",
    "    lfcc = np.pad(lfcc, ((0, 0), (0, target_length - lfcc.shape[1])))\n",
    "else:\n",
    "    lfcc = lfcc[:, :target_length]\n",
    "\n",
    "# Reshape and normalize\n",
    "lfcc = np.expand_dims(lfcc, axis=0)\n",
    "lfcc = np.expand_dims(lfcc, axis=-1)\n",
    "lfcc = (lfcc - mean) / (std + 1e-8)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(lfcc, verbose=0)[0][0]\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(f\"AI_GENERATED (confidence: {{prediction:.2f}})\")\n",
    "else:\n",
    "    print(f\"REAL (confidence: {{1-prediction:.2f}})\")\n",
    "```\n",
    "\n",
    "## Model Specifications\n",
    "- Input: LFCC features (40 x 313 x 1)\n",
    "- Output: Binary classification (0 = REAL, 1 = AI_GENERATED)\n",
    "- Sample Rate: 16000 Hz\n",
    "- Max Duration: 10 seconds\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{final_dir}/USAGE.md', 'w') as f:\n",
    "    f.write(usage_guide)\n",
    "\n",
    "print(f\"✓ Created usage guide: {final_dir}/USAGE.md\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991d21e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-09T09:59:53.818286Z",
     "iopub.status.idle": "2026-02-09T09:59:53.818756Z",
     "shell.execute_reply": "2026-02-09T09:59:53.818629Z",
     "shell.execute_reply.started": "2026-02-09T09:59:53.818613Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Block 7: Standalone Inference Script\n",
    "Ready-to-use script for making predictions on new audio files.\n",
    "This can be used directly in your hackathon submission.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import json\n",
    "import argparse\n",
    "from tensorflow import keras\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AudioAnalyzer:\n",
    "    \"\"\"Analyzes audio files to extract interpretable features.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=16000):\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def analyze_audio(self, audio_path):\n",
    "        \"\"\"Extract audio features for explanation.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=10)\n",
    "            \n",
    "            # Pitch analysis\n",
    "            pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "            pitch_values = []\n",
    "            for t in range(pitches.shape[1]):\n",
    "                index = magnitudes[:, t].argmax()\n",
    "                pitch = pitches[index, t]\n",
    "                if pitch > 0:\n",
    "                    pitch_values.append(pitch)\n",
    "            \n",
    "            pitch_variance = np.var(pitch_values) if len(pitch_values) > 0 else 0\n",
    "            pitch_consistency = 1.0 / (1.0 + pitch_variance / 1000)\n",
    "            \n",
    "            # Silence analysis\n",
    "            rms = librosa.feature.rms(y=y)[0]\n",
    "            silence_threshold = np.percentile(rms, 20)\n",
    "            silent_frames = rms < silence_threshold\n",
    "            silence_ratio = np.mean(silent_frames)\n",
    "            \n",
    "            # Breath analysis\n",
    "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "            rms_threshold = np.percentile(rms, 30)\n",
    "            zcr_threshold = np.percentile(zcr, 70)\n",
    "            potential_breaths = (zcr > zcr_threshold) & (rms < rms_threshold)\n",
    "            breath_ratio = np.mean(potential_breaths)\n",
    "            num_breath_events = self._count_events(potential_breaths)\n",
    "            \n",
    "            # Energy analysis\n",
    "            energy_variance = np.var(rms)\n",
    "            energy_consistency = 1.0 / (1.0 + energy_variance * 100)\n",
    "            \n",
    "            # Spectral analysis\n",
    "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "            spectral_bandwidth_std = np.std(spectral_bandwidth)\n",
    "            \n",
    "            return {\n",
    "                'pitch_variance': float(pitch_variance),\n",
    "                'pitch_consistency': float(pitch_consistency),\n",
    "                'silence_ratio': float(silence_ratio),\n",
    "                'breath_ratio': float(breath_ratio),\n",
    "                'num_breath_events': num_breath_events,\n",
    "                'energy_consistency': float(energy_consistency),\n",
    "                'spectral_bandwidth_std': float(spectral_bandwidth_std)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self._get_default_analysis()\n",
    "    \n",
    "    def _count_events(self, boolean_array):\n",
    "        \"\"\"Count number of True intervals.\"\"\"\n",
    "        count = 0\n",
    "        in_interval = False\n",
    "        for val in boolean_array:\n",
    "            if val and not in_interval:\n",
    "                count += 1\n",
    "                in_interval = True\n",
    "            elif not val:\n",
    "                in_interval = False\n",
    "        return count\n",
    "    \n",
    "    def _get_default_analysis(self):\n",
    "        return {\n",
    "            'pitch_variance': 0,\n",
    "            'pitch_consistency': 0,\n",
    "            'silence_ratio': 0,\n",
    "            'breath_ratio': 0,\n",
    "            'num_breath_events': 0,\n",
    "            'energy_consistency': 0,\n",
    "            'spectral_bandwidth_std': 0\n",
    "        }\n",
    "\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    \"\"\"Generates explanations based on audio analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = AudioAnalyzer()\n",
    "    \n",
    "    def generate(self, audio_path, prediction_class, confidence):\n",
    "        \"\"\"Generate human-readable explanation.\"\"\"\n",
    "        analysis = self.analyzer.analyze_audio(audio_path)\n",
    "        \n",
    "        if prediction_class == 'AI_GENERATED':\n",
    "            return self._explain_ai(analysis)\n",
    "        else:\n",
    "            return self._explain_real(analysis)\n",
    "    \n",
    "    def _explain_ai(self, analysis):\n",
    "        reasons = []\n",
    "        \n",
    "        if analysis['pitch_consistency'] > 0.7:\n",
    "            reasons.append(\"unnatural pitch consistency\")\n",
    "        if analysis['pitch_variance'] < 500:\n",
    "            reasons.append(\"limited pitch variation\")\n",
    "        if analysis['num_breath_events'] < 2:\n",
    "            reasons.append(\"absence of natural breath sounds\")\n",
    "        if analysis['energy_consistency'] > 0.7:\n",
    "            reasons.append(\"robotic speech patterns\")\n",
    "        if analysis['spectral_bandwidth_std'] < 200:\n",
    "            reasons.append(\"limited spectral dynamics\")\n",
    "        \n",
    "        if len(reasons) == 0:\n",
    "            return \"Artificial speech patterns detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            return f\"{reasons[0].capitalize()} detected\"\n",
    "        else:\n",
    "            return f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "    \n",
    "    def _explain_real(self, analysis):\n",
    "        reasons = []\n",
    "        \n",
    "        if analysis['pitch_variance'] > 1000:\n",
    "            reasons.append(\"natural pitch variation\")\n",
    "        if analysis['num_breath_events'] >= 2:\n",
    "            reasons.append(\"natural breath patterns\")\n",
    "        if analysis['energy_consistency'] < 0.5:\n",
    "            reasons.append(\"organic energy fluctuations\")\n",
    "        if analysis['spectral_bandwidth_std'] > 300:\n",
    "            reasons.append(\"rich spectral dynamics\")\n",
    "        \n",
    "        if len(reasons) == 0:\n",
    "            return \"Human speech characteristics detected\"\n",
    "        elif len(reasons) == 1:\n",
    "            return f\"{reasons[0].capitalize()} detected\"\n",
    "        else:\n",
    "            return f\"{reasons[0].capitalize()} and {reasons[1]} detected\"\n",
    "\n",
    "\n",
    "class VoiceClassifier:\n",
    "    \"\"\"Main classifier for voice authentication.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='checkpoints/best_model.h5', \n",
    "                 norm_path='normalization_params.npz'):\n",
    "        \"\"\"Initialize classifier with model and parameters.\"\"\"\n",
    "        \n",
    "        # Load model\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load normalization parameters\n",
    "        norm_params = np.load(norm_path)\n",
    "        self.norm_mean = norm_params['mean']\n",
    "        self.norm_std = norm_params['std']\n",
    "        \n",
    "        # Initialize explainer\n",
    "        self.explainer = ExplanationGenerator()\n",
    "        \n",
    "        # Feature extraction parameters\n",
    "        self.sample_rate = 16000\n",
    "        self.n_lfcc = 40\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.max_duration = 10\n",
    "        \n",
    "        # Language mapping\n",
    "        self.language_map = {\n",
    "            'ta': 'Tamil', 'hi': 'Hindi', 'te': 'Telugu',\n",
    "            'ml': 'Malayalam', 'kn': 'Kannada', 'en': 'English'\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        \"\"\"Extract LFCC features from audio.\"\"\"\n",
    "        y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.max_duration)\n",
    "        \n",
    "        lfcc = librosa.feature.mfcc(\n",
    "            y=y, sr=sr, n_mfcc=self.n_lfcc,\n",
    "            n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        target_length = int(self.max_duration * sr / self.hop_length)\n",
    "        \n",
    "        if lfcc.shape[1] < target_length:\n",
    "            pad_width = target_length - lfcc.shape[1]\n",
    "            lfcc = np.pad(lfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            lfcc = lfcc[:, :target_length]\n",
    "        \n",
    "        return lfcc\n",
    "    \n",
    "    def detect_language(self, filename):\n",
    "        \"\"\"Detect language from filename.\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        for code, language in self.language_map.items():\n",
    "            if code in filename_lower:\n",
    "                return language\n",
    "        return \"English\"\n",
    "    \n",
    "    def classify(self, audio_path):\n",
    "        \"\"\"\n",
    "        Classify an audio file as REAL or AI_GENERATED.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with classification results in the required format\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract features\n",
    "            features = self.extract_features(audio_path)\n",
    "            \n",
    "            # Reshape and normalize\n",
    "            features = np.expand_dims(features, axis=0)\n",
    "            features = np.expand_dims(features, axis=-1)\n",
    "            features = (features - self.norm_mean) / (self.norm_std + 1e-8)\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction_proba = self.model.predict(features, verbose=0)[0][0]\n",
    "            \n",
    "            # Determine class and confidence\n",
    "            if prediction_proba > 0.5:\n",
    "                classification = \"AI_GENERATED\"\n",
    "                confidence_score = float(prediction_proba)\n",
    "            else:\n",
    "                classification = \"REAL\"\n",
    "                confidence_score = float(1.0 - prediction_proba)\n",
    "            \n",
    "            # Detect language\n",
    "            filename = os.path.basename(audio_path)\n",
    "            language = self.detect_language(filename)\n",
    "            \n",
    "            # Generate explanation\n",
    "            explanation = self.explainer.generate(audio_path, classification, confidence_score)\n",
    "            \n",
    "            # Return result in required format\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"language\": language,\n",
    "                \"classification\": classification,\n",
    "                \"confidenceScore\": round(confidence_score, 2),\n",
    "                \"explanation\": explanation\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"language\": \"Unknown\",\n",
    "                \"classification\": \"Unknown\",\n",
    "                \"confidenceScore\": 0.0,\n",
    "                \"explanation\": f\"Error processing audio: {str(e)}\"\n",
    "            }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for command-line usage.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Voice Classification: AI vs Human')\n",
    "    parser.add_argument('audio_file', type=str, help='Path to audio file')\n",
    "    parser.add_argument('--model', type=str, default='checkpoints/best_model.h5',\n",
    "                       help='Path to model file')\n",
    "    parser.add_argument('--norm', type=str, default='normalization_params.npz',\n",
    "                       help='Path to normalization parameters')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize classifier\n",
    "    print(\"Initializing classifier...\")\n",
    "    classifier = VoiceClassifier(model_path=args.model, norm_path=args.norm)\n",
    "    \n",
    "    # Classify audio\n",
    "    print(f\"Classifying: {args.audio_file}\")\n",
    "    result = classifier.classify(args.audio_file)\n",
    "    \n",
    "    # Print result\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION RESULT\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(result, indent=2))\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6d76f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9415542,
     "sourceId": 14789581,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10631.966943,
   "end_time": "2026-02-10T04:59:15.332539",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-10T02:02:03.365596",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
